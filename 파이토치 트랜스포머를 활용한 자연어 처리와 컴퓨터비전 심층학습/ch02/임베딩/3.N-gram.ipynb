{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-gram\n",
    "- 가장 기초적인 통계적 언어 모델\n",
    "- 텍스트에서 n개의 연속된 단어 시퀀스를 하나의 단위로 취급하여 특정 단어 시퀀스가 등장할 확률 추정\n",
    "- 입력 텍스트 하나의 토큰 단위 분석 x \n",
    "- N개의 토큰을 묶어서 분석\n",
    "- N이 1,2,3: 유니그램, 바이그램, 트라이그램\n",
    "- 단어의 순서가 중요한 자연어 처리 작업 및 문자열 패턴 분석에 활용\n",
    "\n",
    "\n",
    "##### 즉 n-gram은 텍스트를 연속된 n개의 토큰으로 구성된 시퀀스로 분리하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
      "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
      "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n",
      "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
      "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
      "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def ngrams(sentence, n):\n",
    "    words = sentence.split()\n",
    "    ngrams = zip(*[words[i:] for i in range(n)]) # *을 이용해 unpacking 사용 리스트 값이 분리되어 전달\n",
    "    return list(ngrams)\n",
    "\n",
    "sentence = '안녕하세요 만나서 진심으로 반가워요'\n",
    "\n",
    "unigram = ngrams(sentence, 1)\n",
    "bigram = ngrams(sentence, 2)\n",
    "trigram = ngrams(sentence, 3)\n",
    "\n",
    "print(unigram)\n",
    "print(bigram)\n",
    "print(trigram)\n",
    "\n",
    "unigram = nltk.ngrams(sentence.split(), 1)\n",
    "bigram = nltk.ngrams(sentence.split(), 2)\n",
    "trigram = nltk.ngrams(sentence.split(), 3)\n",
    "\n",
    "print(list(unigram))\n",
    "print(list(bigram))\n",
    "print(list(trigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "- 텍스트 문서에서 특정 단어의 중요도를 계산하는 방법\n",
    "- 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적인 가중치를 의미 ( BoW(Bag-df-Words (문서를 단어의 순서를 무시하고 단순히 단어의 출현 빈도만을 고려하여 표현))에 가중치 부여하는 방법)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 빈도(TF)\n",
    "- 문서 내에서 특정 단어의 빈도수를 내타내는 값\n",
    "\n",
    "#### 문서 빈도(DF)\n",
    "- 한 단어가 얼마나 많은 문서에 나타나는지 의미\n",
    "\n",
    "#### 역문서 빈도(IDF)\n",
    "- 전체 문서 수 / 문서 빈도 로그를 취한 값\n",
    "- 문서 집합에서 해당 단어가 얼마나 드물게 등장하는지를 나타내는 지표\n",
    "- 문서 빈도가 높을수록 해당 단어가 일반적이고 상대적으로 중요하지 않다는 의미가 됨 -> 문서 빈도의 역수를 취해 단어 빈도수가 적을수록 IDF 값이 커지게 보정\n",
    "\n",
    "#### TF-IDF\n",
    "- 앞선 문서 빈도와 역문서 빈도를 곱한 값\n",
    "- 문서 내에 단어가 자주 등장하지만, 전체 문서 내에 해당 단어가 적게 등장하면 TF-IDF 값은 커짐\n",
    "- 즉 한 문서 내에서는 빈번하게 등장하지만, 전체 문서 집합에서는 드물게 등장하는 경우 값이 커짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.2-cp38-cp38-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_py3.8/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_py3.8/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_py3.8/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Using cached scikit_learn-1.3.2-cp38-cp38-macosx_12_0_arm64.whl (9.4 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.2 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 클래스\n",
    "import sklearn\n",
    "\n",
    "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    input='content', # 입력될 데이터 형태\n",
    "    encoding = 'utf-8', # 사용할 텍스트 인코딩 값\n",
    "    lowercase = True, # 입력 데이터 소문자 변환 여부\n",
    "    stop_words = None, # 불용어 처리 (입력된 단어들은 단어 사전 추가 x)\n",
    "    ngram_range = (1,1), # n그램 범위 (최대값, 최소값)\n",
    "    max_df=1.0, # 전체 문서 중 일정 횟수 이상 등장한 단어 불용어 처리\n",
    "    min_df=1, # 전체 문서 중 일정 횟수 미만으로 등장한 단어를 불용어 처리\n",
    "    vocabulary=None, #미리 구축한 단어사전이 있다면 해당 단어 사전 사용\n",
    "    smooth_idf=True, #분모처리 (IDF계산 시 특정 단어가 한번도 등장하지 않는 경우 0이기 때문에 분모에 1을 더하는데 기능 여부)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.39687454 0.39687454 0.         0.79374908\n",
      "  0.2344005 ]\n",
      " [0.61980538 0.         0.         0.         0.61980538 0.\n",
      "  0.48133417]\n",
      " [0.4804584  0.63174505 0.         0.         0.4804584  0.\n",
      "  0.37311881]]\n",
      "{'that': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 계산\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'That movie is famous movie',\n",
    "    'I like that actor',\n",
    "    \"I don't like that actor\"\n",
    "]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(corpus) # 학습\n",
    "tfidf_matrix = tfidf_vectorizer.transform(corpus) #데이터 변환 (fit_transform메서드를 통해 학습과 변환 동시에도 가능) / 실제 문서(corpus)를 TF-IDF 값이 포함된 벡터 공간으로 변환\n",
    "\n",
    "# 학습과 변환을 한 번에 수행하여 TF-IDF 행렬을 생성\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_matrix.toarray()) # 넘파이 배열로 변환 / 각 행은하나의 문서에 해당, 각 열은 단어를 의미\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
