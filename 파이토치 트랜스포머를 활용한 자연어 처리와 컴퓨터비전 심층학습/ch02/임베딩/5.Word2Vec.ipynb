{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "- 단어 간의 유사성 측정하기 위해 분포 가설을 기반 개발\n",
    "    - 분포 가설: 같은 문맥에서 함께 자주 나타나는 단어는 서로 유사한 의미를 가질 가능성이 높다는 가정\n",
    "        - 동시 발생 확률 분포를 이용해 단어 간의 유사성 측정\n",
    "- 유사한 문맥에서 등장하는 단어는 비슷한 벡터 공강산 위치를 갖게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 벡터화\n",
    "- 방법\n",
    "    - 희소 표현: 데이터를 고차원의 벡터로 표현 벡터 대부분 요소가 0으로 채워짐 \n",
    "        - ex) 어휘 사전의 크기가 10,000이고, 특정 문서에 100개의 고유 단어만 포함되어 있다면, 이 문서는 100개의 비-제로 값과 9,900개의 제로 값을 가진 10,000차원 벡터로 표현\n",
    "        - BoW: 순서에 관계없이 단어의 출현 빈도로만 표현하는 것\n",
    "        - 원-핫 인코딩: 각 단어를 고유한 색인 값으로 매핑\n",
    "        - TF-IDF: 단어의 빈도수와 함께 단어가 전체 문서 집합에서 얼마나 고유한지를 함께 고려하는 방법\n",
    "        - 장점: 명확한 관계를 제공\n",
    "        - 단점: 고차원이며, 대부분의 값이 0으로 채워져 있어 메모리와 계산 효율이 낮고 단어의 의미적 관계를 포착하지 못하는 경우가 많음\n",
    "    - 밀집 표현\n",
    "        - Word2Vec\n",
    "        - 장점: 의미적 유사성을 기반으로 한 단어 간의 관계를 잘 포착, 계산 효율이 높음\n",
    "        - 단점: 특징 차원이 구체적으로 어떤 의미를 가지는지 해석하기 어려울 수 있음\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBoW\n",
    "- 주변에 단어를 가지고 중간에 있단 단어 예측하는 방법\n",
    "- 중심 단어(Center Word): 예측할 단어\n",
    "- 주변 단어(Context Word): 예측에 사용되는 단어\n",
    "- sliding window를 이용해 이동해 가며 학습 -> 한 번의 학습으로 여러 개의 중심 단어, 그에 대한 주변 단어 학습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram\n",
    "- 중심 단어를 입력으로 주변 단어를 예측하는 모델\n",
    "- 중심 단어와 주변 단어를 하나의 쌍으로 하여 여러 학습 데이터가 만들어짐\n",
    "- 말뭉치가 커지면 단어 사전 커져 Word2Vec 모델 학습 시 학습 속도 느려지는 문제\n",
    "    - 해결방법\n",
    "        - 계층적 소프트맥스\n",
    "            - 출력층 이진 트리 구조 표현\n",
    "            - 자주 등장하는 단어 상위 노드, 드물게 등장하는 단어 하위 노드\n",
    "        - 네거티브 샘플링\n",
    "            - 전체 단어 집합에서 일부 단어를 샘플링하여 오답 단어로 사용\n",
    "            - 학습 윈도 내에 등장하지 않는 단어 추출 후 정답 단어와 소프트맥스 연산 수행 (전체 단어 확률 계산할 필요가 없어짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 클래스\n",
    "import torch\n",
    "\n",
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings, # 임베딩 수: 이산 변수의 개수로 단어 사전의 크기\n",
    "    embedding_dim,  # 임베딩 차원: 임베딩 벡터의 크기\n",
    "    padding_idx=None, # 패딩 인덱스: 패딩 토큰의 인덱스를 지정 / 병렬 처리를 위해 문장 길이 동일해야하므로 길이를 맞추는 역할 / 임베딩 수보다 작아야 함\n",
    "    max_norm=None, # 노름 타입: 임베딩 벡터의 크기 제한 default: L2 정규화 \n",
    "    norm_type=2.0 # 최대 노름: 임베딩 벡터의 최대 크기 (각 임베딩 벡터의 크기가 최대 노름 값 이상이면 임베딩 벡터를 최대 노름 크기로 잘라냄)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 skip-gram 클래스\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class VanillaSkipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embedding_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding = self.embedding(input_ids)\n",
    "        output = self.linear(embedding)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/joyoungjun/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/joyoungjun/Korpora/nsmc/ratings_test.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m corpus \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(corpus\u001b[39m.\u001b[39mtest)\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[39m=\u001b[39m Okt()\n\u001b[0;32m---> 11\u001b[0m tokens \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mmorphs(review) \u001b[39mfor\u001b[39;00m review \u001b[39min\u001b[39;00m corpus\u001b[39m.\u001b[39mtext]\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(tokens[:\u001b[39m3\u001b[39m])\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m corpus \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(corpus\u001b[39m.\u001b[39mtest)\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[39m=\u001b[39m Okt()\n\u001b[0;32m---> 11\u001b[0m tokens \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39;49mmorphs(review) \u001b[39mfor\u001b[39;00m review \u001b[39min\u001b[39;00m corpus\u001b[39m.\u001b[39mtext]\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(tokens[:\u001b[39m3\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_py3.8/lib/python3.8/site-packages/konlpy/tag/_okt.py:89\u001b[0m, in \u001b[0;36mOkt.morphs\u001b[0;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmorphs\u001b[39m(\u001b[39mself\u001b[39m, phrase, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, stem\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     87\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m [s \u001b[39mfor\u001b[39;00m s, t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos(phrase, norm\u001b[39m=\u001b[39;49mnorm, stem\u001b[39m=\u001b[39;49mstem)]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_py3.8/lib/python3.8/site-packages/konlpy/tag/_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[0;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"POS tagger.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[0;32m---> 71\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjki\u001b[39m.\u001b[39;49mtokenize(\n\u001b[1;32m     72\u001b[0m             phrase,\n\u001b[1;32m     73\u001b[0m             jpype\u001b[39m.\u001b[39;49mjava\u001b[39m.\u001b[39;49mlang\u001b[39m.\u001b[39;49mBoolean(norm),\n\u001b[1;32m     74\u001b[0m             jpype\u001b[39m.\u001b[39;49mjava\u001b[39m.\u001b[39;49mlang\u001b[39m.\u001b[39;49mBoolean(stem))\u001b[39m.\u001b[39mtoArray()\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m join:\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m [t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tokens]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 영화 리뷰 데이터세트 전처리\n",
    "\n",
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "corpus = Korpora.load('nsmc')\n",
    "corpus = pd.DataFrame(corpus.test)\n",
    "\n",
    "tokenizer = Okt()\n",
    "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 사전 구축\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens): # n_vocab: 구축할 단어 사전의 크기 초과 시 가장 많이 등장한 토큰 순서로 사전 구축, special_tokens: 특별한 의미를 갖는 토큰 <unk>:oov대응 토큰\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens) # 업데이트 진행\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
