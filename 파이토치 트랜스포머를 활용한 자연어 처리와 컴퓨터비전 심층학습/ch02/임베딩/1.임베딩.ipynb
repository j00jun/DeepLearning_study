{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 벡터화\n",
    "- 토큰화 만으로는 모델 학습 x\n",
    "- 텍스트를 숫자로 변환하는 과정\n",
    "- 종류\n",
    "    - 빈도 기반 방법\n",
    "        - 원-핫 인코딩\n",
    "            - 각 단어를 고유한 색인 값으로 매핑 (고유한 값에 대한 인덱스 부여) / 색인 위치 1로 표시 나머지 0으로 표시\n",
    "        - 빈도 벡터화\n",
    "            - 문서에서 단어의 빈도수를 세어 해당 단어의 빈도를 벡터로 표현하는 방식\n",
    "        - 단점\n",
    "            - 벡터의 희소성\n",
    "    - 워드 인베딩\n",
    "        - Word2vec\n",
    "        - fastText\n",
    "        - 단어를 고정된 길이의 실수 벡터로 표현, 단어의 의미를 벡터 공간에서 다른 단어와의 상대적 위치로 표현해 단어 간의 관계 추론\n",
    "        - 인공 신경망을 활용해 동적 임베딩 기법을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화 후 ID 변환\n",
    "- 기본적인 변환 과정: 토큰화 후 ID 변환은 텍스트를 더 작은 단위(예: 단어, 서브워드)로 나눈 다음, 각 토큰에 고유한 숫자 ID를 할당하는 과정입니다.\n",
    "- 정보의 수준: 이 과정에서 각 토큰은 고유한 ID로 표현됩니다. 하지만 이 ID는 토큰의 의미나 문맥 정보를 내포하지 않습니다. 즉, 단어 간의 관계나 문맥상의 뉘앙스는 이 ID 자체에서 파악할 수 없습니다.\n",
    "#### 워드 인베딩\n",
    "- 의미적 표현: 워드 인베딩은 각 토큰(단어)을 고차원의 벡터로 표현합니다. 이 벡터는 단어의 의미적, 문맥적 특성을 포함합니다.\n",
    "- 정보의 깊이: 워드 인베딩은 단어들 사이의 유사성이나 관계를 벡터 공간 상에서 표현할 수 있습니다. 예를 들어, 의미적으로 유사한 단어들은 벡터 공간에서 서로 가깝게 위치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
