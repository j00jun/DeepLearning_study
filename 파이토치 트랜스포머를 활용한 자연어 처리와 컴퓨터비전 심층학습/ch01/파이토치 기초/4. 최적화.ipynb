{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적화\n",
    "- 목적 함수의 결괏값을 최적화하는 변수를 찾는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사 하강법\n",
    "- 함수의 기울기가 낮은 곳으로 계속 이동시켜 극값에 도달할 때 까지 반복하는 알고리즘\n",
    "$$θ_{new} = θ_{old} - α∇J(θ_{old})$$\n",
    "\n",
    "$$ α∇J(θ_{old}): 기울기 α: step size $$\n",
    "\n",
    "- 기울기가 0인 방향으로 학습 진행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중치 갱신 방법\n",
    "$$ W_{new} = W_{old} - α * ∇L(W_{old}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최적화 문제\n",
    "- 학습률을 너무 낮게 잡으면 \n",
    "    - 극소 지점을 넘지 못해 로컬 미니멈 값으로 가중치 결정될 수 있음\n",
    "    - 안장점이 존재하는 함수에도 적절한 가중치를 찾을 수 없음\n",
    "- 경사 하강법 이외의 최적화 알고리즘 종류\n",
    "    - Momentum\n",
    "    - Adagrad\n",
    "    - Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단순 선형 회귀: 넘파이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 선언\n",
    "import numpy as np\n",
    "\n",
    "x = np.array(\n",
    "    [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10],\n",
    "    [11], [12], [13], [14], [15], [16], [17], [18], [19], [20],\n",
    "    [21], [22], [23], [24], [25], [26], [27], [28], [29], [30]]\n",
    ")\n",
    "y = np.array(\n",
    "    [[0.94], [1.98], [2.88], [3.92], [3.96], [4.55], [5.64], [6.3], [7.44], [9.1],\n",
    "    [8.46], [9.5], [10.67], [11.16], [14], [11.83], [14.4], [14.25], [16.2], [16.32],\n",
    "    [17.46], [19.8], [18], [21.34], [22], [22.5], [24.57], [26.04], [21.6], [28.8]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 초기화(넘파이)\n",
    "weight = 0.0\n",
    "bias = 0.0\n",
    "learning_rate = 0.005 # 너무 높으면 갱신 값이 발산해버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1000, Weight : 0.872, Bias : -0.290, Cost : 1.377\n",
      "Epoch : 2000, Weight : 0.877, Bias : -0.391, Cost : 1.373\n",
      "Epoch : 3000, Weight : 0.878, Bias : -0.422, Cost : 1.372\n",
      "Epoch : 4000, Weight : 0.879, Bias : -0.432, Cost : 1.372\n",
      "Epoch : 5000, Weight : 0.879, Bias : -0.435, Cost : 1.372\n",
      "Epoch : 6000, Weight : 0.879, Bias : -0.436, Cost : 1.372\n",
      "Epoch : 7000, Weight : 0.879, Bias : -0.436, Cost : 1.372\n",
      "Epoch : 8000, Weight : 0.879, Bias : -0.436, Cost : 1.372\n",
      "Epoch : 9000, Weight : 0.879, Bias : -0.436, Cost : 1.372\n",
      "Epoch : 10000, Weight : 0.879, Bias : -0.436, Cost : 1.372\n"
     ]
    }
   ],
   "source": [
    "# 에포크 설정\n",
    "for epoch in range(10000):\n",
    "    # 가설 손실함수 선언\n",
    "    y_hat = weight * x + bias \n",
    "    cost = ((y - y_hat) ** 2).mean()\n",
    "    # 가중치와 편향 갱신 / 손실 함수의 기울기를 계산하여 가중치와 편향을 업데이트\n",
    "    weight = weight - learning_rate * ((y_hat - y) * x).mean() \n",
    "    bias = bias - learning_rate * (y_hat - y).mean()\n",
    "    # 학습 기록\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, Weight : {weight:.3f}, Bias : {bias:.3f}, Cost : {cost:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단순 선형 회귀: 파이토치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프레임워크 선언 및 데이터 선언\n",
    "import torch\n",
    "from torch import optim # optim모듈을 통해 다양한 최적화 함수를 간단하게 사용 가능\n",
    "\n",
    "x = torch.FloatTensor([\n",
    "    [1], [2], [3], [4], [5], [6], [7], [8], [9], [10],\n",
    "    [11], [12], [13], [14], [15], [16], [17], [18], [19], [20],\n",
    "    [21], [22], [23], [24], [25], [26], [27], [28], [29], [30]\n",
    "])\n",
    "y = torch.FloatTensor([\n",
    "    [0.94], [1.98], [2.88], [3.92], [3.96], [4.55], [5.64], [6.3], [7.44], [9.1],\n",
    "    [8.46], [9.5], [10.67], [11.16], [14], [11.83], [14.4], [14.25], [16.2], [16.32],\n",
    "    [17.46], [19.8], [18], [21.34], [22], [22.5], [24.57], [26.04], [21.6], [28.8]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 초기화\n",
    "weight = torch.zeros(1, requires_grad=True) # requires_grad: 텐서 연산 추척, 역전파 메서드 호출 기울기 계산 저장(자동 미분)\n",
    "bias = torch.zeros(1, requires_grad=True)\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.SGD(    \n",
    "<blockquote>params, # 최적화될 매개변수들의 리스트</blockquote>\n",
    "<blockquote>lr, # 학습률  </blockquote>\n",
    "<blockquote>**kwargs # 추가적인 옵션  </blockquote>\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 선언\n",
    "optimizer = optim.SGD([weight, bias], lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1000, Weight : 0.864, Bias : -0.138, Cost : 1.393\n",
      "Epoch : 2000, Weight : 0.870, Bias : -0.251, Cost : 1.380\n",
      "Epoch : 3000, Weight : 0.873, Bias : -0.321, Cost : 1.375\n",
      "Epoch : 4000, Weight : 0.875, Bias : -0.364, Cost : 1.373\n",
      "Epoch : 5000, Weight : 0.877, Bias : -0.391, Cost : 1.373\n",
      "Epoch : 6000, Weight : 0.878, Bias : -0.408, Cost : 1.372\n",
      "Epoch : 7000, Weight : 0.878, Bias : -0.419, Cost : 1.372\n",
      "Epoch : 8000, Weight : 0.878, Bias : -0.425, Cost : 1.372\n",
      "Epoch : 9000, Weight : 0.879, Bias : -0.429, Cost : 1.372\n",
      "Epoch : 10000, Weight : 0.879, Bias : -0.432, Cost : 1.372\n"
     ]
    }
   ],
   "source": [
    "# 에폭, 가설, 손실 함수 선언\n",
    "for epoch in range(10000):\n",
    "    hypothesis = x * weight + bias\n",
    "    cost = torch.mean((hypothesis - y) ** 2)\n",
    "\n",
    "    # 가중치와 편향 갱신\n",
    "    optimizer.zero_grad() # optimizer 변수에 포함시킨 매개변수들 기울기 0으로 초기화\n",
    "    cost.backward() # 역전파 수행 이를 통해 매개변수들의 기울기 새로 계산\n",
    "    optimizer.step() # learning rate 값 반영한 확률적 경사 하강법 연산 적용\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, Weight : {weight.item():.3f}, Bias : {bias.item():.3f}, Cost : {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1\n",
      "Step [1] : Gradient : None, Weight : 0.00000\n",
      "Step [2] : Gradient : None, Weight : 0.00000\n",
      "Step [3] : Gradient : tensor([-540.4854]), Weight : 0.00000\n",
      "Step [4] : Gradient : tensor([-540.4854]), Weight : 0.54049\n",
      "Epoch :    2\n",
      "Step [1] : Gradient : tensor([-540.4854]), Weight : 0.54049\n",
      "Step [2] : Gradient : None, Weight : 0.54049\n",
      "Step [3] : Gradient : tensor([-198.9818]), Weight : 0.54049\n",
      "Step [4] : Gradient : tensor([-198.9818]), Weight : 0.73947\n",
      "Epoch :    3\n",
      "Step [1] : Gradient : tensor([-198.9818]), Weight : 0.73947\n",
      "Step [2] : Gradient : None, Weight : 0.73947\n",
      "Step [3] : Gradient : tensor([-73.2604]), Weight : 0.73947\n",
      "Step [4] : Gradient : tensor([-73.2604]), Weight : 0.81273\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 초기화\n",
    "weight = torch.zeros(1, requires_grad=True)\n",
    "bias = torch.zeros(1, requires_grad=True)\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 최적화 선언\n",
    "optimizer = optim.SGD([weight, bias], lr = learning_rate)\n",
    "\n",
    "for epoch in range(3):\n",
    "    hypothesis = weight * x + bias\n",
    "    cost = torch.mean((hypothesis - y) ** 2)\n",
    "    \n",
    "    print(f\"Epoch : {epoch+1:4d}\")\n",
    "    print(f\"Step [1] : Gradient : {weight.grad}, Weight : {weight.item():.5f}\")\n",
    "\n",
    "    # 기울기가 없기 때문에 zero_grad 초기값과 동일\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Step [2] : Gradient : {weight.grad}, Weight : {weight.item():.5f}\")\n",
    "    # 역전파로 기울기 계산\n",
    "    cost.backward()\n",
    "    print(f\"Step [3] : Gradient : {weight.grad}, Weight : {weight.item():.5f}\")\n",
    "    # 역전파로 인한 기울기 계산 후 weight값 반영\n",
    "    optimizer.step()\n",
    "    print(f\"Step [4] : Gradient : {weight.grad}, Weight : {weight.item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 신경망 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 변환 클래스\n",
    "layer = torch.nn.Linear(\n",
    "    in_features # 입력 차원크기\n",
    "    out_features # 출력 차원 크기\n",
    "    bias = True # bias 값 포함 여부\n",
    "    device = None # 레이어 생성될 device지정 / 데이터와 모델은 같은 장치에 있어야함\n",
    "    dtype = None  # 레이어의 데이터 타입을 설정 / 데이터와 모델 레이어는 동일한 데이터 타입\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# weight = torch.zeros(1, requires_grad=True)\n",
    "# bias = torch.zeros(1, requires_grad=True)\n",
    "model = nn.Linear(1, 1, bias=True) # 위 코드를 선형 변환 클래스로 대체해 모델 구성\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순방향 계산\n",
    "for epoch in range(10000):\n",
    "    output = model(x) # x전달 후 output반환\n",
    "    cost = criterion(output, y) # 입력과 목표를 전달받아 순방향 연산 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1000, Model : [Parameter containing:\n",
      "tensor([[0.8448]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2565], requires_grad=True)], Cost : 1.486\n",
      "Epoch : 2000, Model : [Parameter containing:\n",
      "tensor([[0.8577]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0050], requires_grad=True)], Cost : 1.416\n",
      "Epoch : 3000, Model : [Parameter containing:\n",
      "tensor([[0.8657]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1678], requires_grad=True)], Cost : 1.389\n",
      "Epoch : 4000, Model : [Parameter containing:\n",
      "tensor([[0.8707]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2691], requires_grad=True)], Cost : 1.379\n",
      "Epoch : 5000, Model : [Parameter containing:\n",
      "tensor([[0.8738]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3321], requires_grad=True)], Cost : 1.375\n",
      "Epoch : 6000, Model : [Parameter containing:\n",
      "tensor([[0.8757]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3713], requires_grad=True)], Cost : 1.373\n",
      "Epoch : 7000, Model : [Parameter containing:\n",
      "tensor([[0.8769]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3957], requires_grad=True)], Cost : 1.373\n",
      "Epoch : 8000, Model : [Parameter containing:\n",
      "tensor([[0.8777]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4109], requires_grad=True)], Cost : 1.372\n",
      "Epoch : 9000, Model : [Parameter containing:\n",
      "tensor([[0.8781]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4204], requires_grad=True)], Cost : 1.372\n",
      "Epoch : 10000, Model : [Parameter containing:\n",
      "tensor([[0.8784]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4262], requires_grad=True)], Cost : 1.372\n"
     ]
    }
   ],
   "source": [
    "# 신경망 패키지\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "x = torch.FloatTensor([\n",
    "    [1], [2], [3], [4], [5], [6], [7], [8], [9], [10],\n",
    "    [11], [12], [13], [14], [15], [16], [17], [18], [19], [20],\n",
    "    [21], [22], [23], [24], [25], [26], [27], [28], [29], [30]\n",
    "])\n",
    "y = torch.FloatTensor([\n",
    "    [0.94], [1.98], [2.88], [3.92], [3.96], [4.55], [5.64], [6.3], [7.44], [9.1],\n",
    "    [8.46], [9.5], [10.67], [11.16], [14], [11.83], [14.4], [14.25], [16.2], [16.32],\n",
    "    [17.46], [19.8], [18], [21.34], [22], [22.5], [24.57], [26.04], [21.6], [28.8]\n",
    "])\n",
    "\n",
    "model = nn.Linear(1, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    output = model(x)\n",
    "    cost = criterion(output, y)\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, Model : {list(model.parameters())}, Cost : {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
