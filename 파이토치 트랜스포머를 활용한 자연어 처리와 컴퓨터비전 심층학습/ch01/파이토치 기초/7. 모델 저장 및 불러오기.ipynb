{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 함수\n",
    "import torch \n",
    "\n",
    "torch.save(\n",
    "    model,\n",
    "    path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Module:\n",
      "    r\"\"\"Base class for all neural network modules.\n",
      "\n",
      "    Your models should also subclass this class.\n",
      "\n",
      "    Modules can also contain other Modules, allowing to nest them in\n",
      "    a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "        import torch.nn as nn\n",
      "        import torch.nn.functional as F\n",
      "\n",
      "        class Model(nn.Module):\n",
      "            def __init__(self):\n",
      "                super().__init__()\n",
      "                self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "                self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "            def forward(self, x):\n",
      "                x = F.relu(self.conv1(x))\n",
      "                return F.relu(self.conv2(x))\n",
      "\n",
      "    Submodules assigned in this way will be registered, and will have their\n",
      "    parameters converted too when you call :meth:`to`, etc.\n",
      "\n",
      "    .. note::\n",
      "        As per the example above, an ``__init__()`` call to the parent class\n",
      "        must be made before assignment on the child.\n",
      "\n",
      "    :ivar training: Boolean represents whether this module is in training or\n",
      "                    evaluation mode.\n",
      "    :vartype training: bool\n",
      "    \"\"\"\n",
      "\n",
      "    dump_patches: bool = False\n",
      "\n",
      "    _version: int = 1\n",
      "    r\"\"\"This allows better BC support for :meth:`load_state_dict`. In\n",
      "    :meth:`state_dict`, the version number will be saved as in the attribute\n",
      "    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a\n",
      "    dictionary with keys that follow the naming convention of state dict. See\n",
      "    ``_load_from_state_dict`` on how to use this information in loading.\n",
      "\n",
      "    If new parameters/buffers are added/removed from a module, this number shall\n",
      "    be bumped, and the module's `_load_from_state_dict` method can compare the\n",
      "    version number and do appropriate changes if the state dict is from before\n",
      "    the change.\"\"\"\n",
      "\n",
      "    training: bool\n",
      "    _parameters: Dict[str, Optional[Parameter]]\n",
      "    _buffers: Dict[str, Optional[Tensor]]\n",
      "    _non_persistent_buffers_set: Set[str]\n",
      "    _backward_pre_hooks: Dict[int, Callable]\n",
      "    _backward_hooks: Dict[int, Callable]\n",
      "    _is_full_backward_hook: Optional[bool]\n",
      "    _forward_hooks: Dict[int, Callable]\n",
      "    # Marks whether the corresponding _forward_hooks accept kwargs or not.\n",
      "    # As JIT does not support Set[int], this dict is used as a set, where all\n",
      "    # hooks represented in this dict accept kwargs.\n",
      "    _forward_hooks_with_kwargs: Dict[int, bool]\n",
      "    # forward hooks that should always be called even if an exception is raised\n",
      "    _forward_hooks_always_called: Dict[int, bool]\n",
      "    _forward_pre_hooks: Dict[int, Callable]\n",
      "    # Marks whether the corresponding _forward_hooks accept kwargs or not.\n",
      "    # As JIT does not support Set[int], this dict is used as a set, where all\n",
      "    # hooks represented in this dict accept kwargs.\n",
      "    _forward_pre_hooks_with_kwargs: Dict[int, bool]\n",
      "    _state_dict_hooks: Dict[int, Callable]\n",
      "    _load_state_dict_pre_hooks: Dict[int, Callable]\n",
      "    _state_dict_pre_hooks: Dict[int, Callable]\n",
      "    _load_state_dict_post_hooks: Dict[int, Callable]\n",
      "    _modules: Dict[str, Optional['Module']]\n",
      "    call_super_init: bool = False\n",
      "    _compiled_call_impl : Optional[Callable] = None\n",
      "\n",
      "\n",
      "\n",
      "    def __init__(self, *args, **kwargs) -> None:\n",
      "        \"\"\"\n",
      "        Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "        \"\"\"\n",
      "\n",
      "        torch._C._log_api_usage_once(\"python.nn_module\")\n",
      "\n",
      "        # Backward compatibility: no args used to be allowed when call_super_init=False\n",
      "        if self.call_super_init is False and bool(kwargs):\n",
      "            raise TypeError(\"{}.__init__() got an unexpected keyword argument '{}'\"\n",
      "                            \"\".format(type(self).__name__, next(iter(kwargs))))\n",
      "\n",
      "        if self.call_super_init is False and bool(args):\n",
      "            raise TypeError(f\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\n",
      "                            \" given\")\n",
      "\n",
      "        \"\"\"\n",
      "        Calls super().__setattr__('a', a) instead of the typical self.a = a\n",
      "        to avoid Module.__setattr__ overhead. Module's __setattr__ has special\n",
      "        handling for parameters, submodules, and buffers but simply calls into\n",
      "        super().__setattr__ for all other attributes.\n",
      "        \"\"\"\n",
      "        super().__setattr__('training', True)\n",
      "        super().__setattr__('_parameters', OrderedDict())\n",
      "        super().__setattr__('_buffers', OrderedDict())\n",
      "        super().__setattr__('_non_persistent_buffers_set', set())\n",
      "        super().__setattr__('_backward_pre_hooks', OrderedDict())\n",
      "        super().__setattr__('_backward_hooks', OrderedDict())\n",
      "        super().__setattr__('_is_full_backward_hook', None)\n",
      "        super().__setattr__('_forward_hooks', OrderedDict())\n",
      "        super().__setattr__('_forward_hooks_with_kwargs', OrderedDict())\n",
      "        super().__setattr__('_forward_hooks_always_called', OrderedDict())\n",
      "        super().__setattr__('_forward_pre_hooks', OrderedDict())\n",
      "        super().__setattr__('_forward_pre_hooks_with_kwargs', OrderedDict())\n",
      "        super().__setattr__('_state_dict_hooks', OrderedDict())\n",
      "        super().__setattr__('_state_dict_pre_hooks', OrderedDict())\n",
      "        super().__setattr__('_load_state_dict_pre_hooks', OrderedDict())\n",
      "        super().__setattr__('_load_state_dict_post_hooks', OrderedDict())\n",
      "        super().__setattr__('_modules', OrderedDict())\n",
      "\n",
      "        if self.call_super_init:\n",
      "            super().__init__(*args, **kwargs)\n",
      "\n",
      "    forward: Callable[..., Any] = _forward_unimplemented\n",
      "\n",
      "    def register_buffer(self, name: str, tensor: Optional[Tensor], persistent: bool = True) -> None:\n",
      "        r\"\"\"Adds a buffer to the module.\n",
      "\n",
      "        This is typically used to register a buffer that should not to be\n",
      "        considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "        is not a parameter, but is part of the module's state. Buffers, by\n",
      "        default, are persistent and will be saved alongside parameters. This\n",
      "        behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "        only difference between a persistent buffer and a non-persistent buffer\n",
      "        is that the latter will not be a part of this module's\n",
      "        :attr:`state_dict`.\n",
      "\n",
      "        Buffers can be accessed as attributes using given names.\n",
      "\n",
      "        Args:\n",
      "            name (str): name of the buffer. The buffer can be accessed\n",
      "                from this module using the given name\n",
      "            tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "                that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "                the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "            persistent (bool): whether the buffer is part of this module's\n",
      "                :attr:`state_dict`.\n",
      "\n",
      "        Example::\n",
      "\n",
      "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "            >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "\n",
      "        \"\"\"\n",
      "        if persistent is False and isinstance(self, torch.jit.ScriptModule):\n",
      "            raise RuntimeError(\"ScriptModule does not support non-persistent buffers\")\n",
      "\n",
      "        if '_buffers' not in self.__dict__:\n",
      "            raise AttributeError(\n",
      "                \"cannot assign buffer before Module.__init__() call\")\n",
      "        elif not isinstance(name, str):\n",
      "            raise TypeError(f\"buffer name should be a string. Got {torch.typename(name)}\")\n",
      "        elif '.' in name:\n",
      "            raise KeyError(\"buffer name can't contain \\\".\\\"\")\n",
      "        elif name == '':\n",
      "            raise KeyError(\"buffer name can't be empty string \\\"\\\"\")\n",
      "        elif hasattr(self, name) and name not in self._buffers:\n",
      "            raise KeyError(f\"attribute '{name}' already exists\")\n",
      "        elif tensor is not None and not isinstance(tensor, torch.Tensor):\n",
      "            raise TypeError(f\"cannot assign '{torch.typename(tensor)}' object to buffer '{name}' \"\n",
      "                            \"(torch Tensor or None required)\"\n",
      "                            )\n",
      "        else:\n",
      "            for hook in _global_buffer_registration_hooks.values():\n",
      "                output = hook(self, name, tensor)\n",
      "                if output is not None:\n",
      "                    tensor = output\n",
      "            self._buffers[name] = tensor\n",
      "            if persistent:\n",
      "                self._non_persistent_buffers_set.discard(name)\n",
      "            else:\n",
      "                self._non_persistent_buffers_set.add(name)\n",
      "\n",
      "    def register_parameter(self, name: str, param: Optional[Parameter]) -> None:\n",
      "        r\"\"\"Adds a parameter to the module.\n",
      "\n",
      "        The parameter can be accessed as an attribute using given name.\n",
      "\n",
      "        Args:\n",
      "            name (str): name of the parameter. The parameter can be accessed\n",
      "                from this module using the given name\n",
      "            param (Parameter or None): parameter to be added to the module. If\n",
      "                ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "                are ignored. If ``None``, the parameter is **not** included in the\n",
      "                module's :attr:`state_dict`.\n",
      "        \"\"\"\n",
      "        if '_parameters' not in self.__dict__:\n",
      "            raise AttributeError(\n",
      "                \"cannot assign parameter before Module.__init__() call\")\n",
      "\n",
      "        elif not isinstance(name, str):\n",
      "            raise TypeError(f\"parameter name should be a string. Got {torch.typename(name)}\")\n",
      "        elif '.' in name:\n",
      "            raise KeyError(\"parameter name can't contain \\\".\\\"\")\n",
      "        elif name == '':\n",
      "            raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n",
      "        elif hasattr(self, name) and name not in self._parameters:\n",
      "            raise KeyError(f\"attribute '{name}' already exists\")\n",
      "\n",
      "        if param is None:\n",
      "            self._parameters[name] = None\n",
      "        elif not isinstance(param, Parameter):\n",
      "            raise TypeError(f\"cannot assign '{torch.typename(param)}' object to parameter '{name}' \"\n",
      "                            \"(torch.nn.Parameter or None required)\"\n",
      "                            )\n",
      "        elif param.grad_fn:\n",
      "            raise ValueError(\n",
      "                f\"Cannot assign non-leaf Tensor to parameter '{name}'. Model \"\n",
      "                f\"parameters must be created explicitly. To express '{name}' \"\n",
      "                \"as a function of another Tensor, compute the value in \"\n",
      "                \"the forward() method.\")\n",
      "        else:\n",
      "            for hook in _global_parameter_registration_hooks.values():\n",
      "                output = hook(self, name, param)\n",
      "                if output is not None:\n",
      "                    param = output\n",
      "            self._parameters[name] = param\n",
      "\n",
      "    def add_module(self, name: str, module: Optional['Module']) -> None:\n",
      "        r\"\"\"Adds a child module to the current module.\n",
      "\n",
      "        The module can be accessed as an attribute using the given name.\n",
      "\n",
      "        Args:\n",
      "            name (str): name of the child module. The child module can be\n",
      "                accessed from this module using the given name\n",
      "            module (Module): child module to be added to the module.\n",
      "        \"\"\"\n",
      "        if not isinstance(module, Module) and module is not None:\n",
      "            raise TypeError(f\"{torch.typename(module)} is not a Module subclass\")\n",
      "        elif not isinstance(name, str):\n",
      "            raise TypeError(f\"module name should be a string. Got {torch.typename(name)}\")\n",
      "        elif hasattr(self, name) and name not in self._modules:\n",
      "            raise KeyError(f\"attribute '{name}' already exists\")\n",
      "        elif '.' in name:\n",
      "            raise KeyError(f\"module name can't contain \\\".\\\", got: {name}\")\n",
      "        elif name == '':\n",
      "            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
      "        for hook in _global_module_registration_hooks.values():\n",
      "            output = hook(self, name, module)\n",
      "            if output is not None:\n",
      "                module = output\n",
      "        self._modules[name] = module\n",
      "\n",
      "    def register_module(self, name: str, module: Optional['Module']) -> None:\n",
      "        r\"\"\"Alias for :func:`add_module`.\"\"\"\n",
      "        self.add_module(name, module)\n",
      "\n",
      "    def get_submodule(self, target: str) -> \"Module\":\n",
      "        \"\"\"\n",
      "        Returns the submodule given by ``target`` if it exists,\n",
      "        otherwise throws an error.\n",
      "\n",
      "        For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "        looks like this:\n",
      "\n",
      "        .. code-block:: text\n",
      "\n",
      "            A(\n",
      "                (net_b): Module(\n",
      "                    (net_c): Module(\n",
      "                        (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "                    )\n",
      "                    (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "                )\n",
      "            )\n",
      "\n",
      "        (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "        submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "        and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "\n",
      "        To check whether or not we have the ``linear`` submodule, we\n",
      "        would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "        we have the ``conv`` submodule, we would call\n",
      "        ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "\n",
      "        The runtime of ``get_submodule`` is bounded by the degree\n",
      "        of module nesting in ``target``. A query against\n",
      "        ``named_modules`` achieves the same result, but it is O(N) in\n",
      "        the number of transitive modules. So, for a simple check to see\n",
      "        if some submodule exists, ``get_submodule`` should always be\n",
      "        used.\n",
      "\n",
      "        Args:\n",
      "            target: The fully-qualified string name of the submodule\n",
      "                to look for. (See above example for how to specify a\n",
      "                fully-qualified string.)\n",
      "\n",
      "        Returns:\n",
      "            torch.nn.Module: The submodule referenced by ``target``\n",
      "\n",
      "        Raises:\n",
      "            AttributeError: If the target string references an invalid\n",
      "                path or resolves to something that is not an\n",
      "                ``nn.Module``\n",
      "        \"\"\"\n",
      "        if target == \"\":\n",
      "            return self\n",
      "\n",
      "        atoms: List[str] = target.split(\".\")\n",
      "        mod: torch.nn.Module = self\n",
      "\n",
      "        for item in atoms:\n",
      "\n",
      "            if not hasattr(mod, item):\n",
      "                raise AttributeError(mod._get_name() + \" has no \"\n",
      "                                     \"attribute `\" + item + \"`\")\n",
      "\n",
      "            mod = getattr(mod, item)\n",
      "\n",
      "            if not isinstance(mod, torch.nn.Module):\n",
      "                raise AttributeError(\"`\" + item + \"` is not \"\n",
      "                                     \"an nn.Module\")\n",
      "\n",
      "        return mod\n",
      "\n",
      "    def get_parameter(self, target: str) -> \"Parameter\":\n",
      "        \"\"\"\n",
      "        Returns the parameter given by ``target`` if it exists,\n",
      "        otherwise throws an error.\n",
      "\n",
      "        See the docstring for ``get_submodule`` for a more detailed\n",
      "        explanation of this method's functionality as well as how to\n",
      "        correctly specify ``target``.\n",
      "\n",
      "        Args:\n",
      "            target: The fully-qualified string name of the Parameter\n",
      "                to look for. (See ``get_submodule`` for how to specify a\n",
      "                fully-qualified string.)\n",
      "\n",
      "        Returns:\n",
      "            torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "\n",
      "        Raises:\n",
      "            AttributeError: If the target string references an invalid\n",
      "                path or resolves to something that is not an\n",
      "                ``nn.Parameter``\n",
      "        \"\"\"\n",
      "        module_path, _, param_name = target.rpartition(\".\")\n",
      "\n",
      "        mod: torch.nn.Module = self.get_submodule(module_path)\n",
      "\n",
      "        if not hasattr(mod, param_name):\n",
      "            raise AttributeError(mod._get_name() + \" has no attribute `\"\n",
      "                                 + param_name + \"`\")\n",
      "\n",
      "        param: torch.nn.Parameter = getattr(mod, param_name)\n",
      "\n",
      "        if not isinstance(param, torch.nn.Parameter):\n",
      "            raise AttributeError(\"`\" + param_name + \"` is not an \"\n",
      "                                 \"nn.Parameter\")\n",
      "\n",
      "        return param\n",
      "\n",
      "    def get_buffer(self, target: str) -> \"Tensor\":\n",
      "        \"\"\"\n",
      "        Returns the buffer given by ``target`` if it exists,\n",
      "        otherwise throws an error.\n",
      "\n",
      "        See the docstring for ``get_submodule`` for a more detailed\n",
      "        explanation of this method's functionality as well as how to\n",
      "        correctly specify ``target``.\n",
      "\n",
      "        Args:\n",
      "            target: The fully-qualified string name of the buffer\n",
      "                to look for. (See ``get_submodule`` for how to specify a\n",
      "                fully-qualified string.)\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: The buffer referenced by ``target``\n",
      "\n",
      "        Raises:\n",
      "            AttributeError: If the target string references an invalid\n",
      "                path or resolves to something that is not a\n",
      "                buffer\n",
      "        \"\"\"\n",
      "        module_path, _, buffer_name = target.rpartition(\".\")\n",
      "\n",
      "        mod: torch.nn.Module = self.get_submodule(module_path)\n",
      "\n",
      "        if not hasattr(mod, buffer_name):\n",
      "            raise AttributeError(mod._get_name() + \" has no attribute `\"\n",
      "                                 + buffer_name + \"`\")\n",
      "\n",
      "        buffer: torch.Tensor = getattr(mod, buffer_name)\n",
      "\n",
      "        if buffer_name not in mod._buffers:\n",
      "            raise AttributeError(\"`\" + buffer_name + \"` is not a buffer\")\n",
      "\n",
      "        return buffer\n",
      "\n",
      "    def get_extra_state(self) -> Any:\n",
      "        \"\"\"\n",
      "        Returns any extra state to include in the module's state_dict.\n",
      "        Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "        if you need to store extra state. This function is called when building the\n",
      "        module's `state_dict()`.\n",
      "\n",
      "        Note that extra state should be picklable to ensure working serialization\n",
      "        of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "        for serializing Tensors; other objects may break backwards compatibility if\n",
      "        their serialized pickled form changes.\n",
      "\n",
      "        Returns:\n",
      "            object: Any extra state to store in the module's state_dict\n",
      "        \"\"\"\n",
      "        raise RuntimeError(\n",
      "            \"Reached a code path in Module.get_extra_state() that should never be called. \"\n",
      "            \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n",
      "            \"to report this bug.\")\n",
      "\n",
      "    def set_extra_state(self, state: Any):\n",
      "        \"\"\"\n",
      "        This function is called from :func:`load_state_dict` to handle any extra state\n",
      "        found within the `state_dict`. Implement this function and a corresponding\n",
      "        :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "        `state_dict`.\n",
      "\n",
      "        Args:\n",
      "            state (dict): Extra state from the `state_dict`\n",
      "        \"\"\"\n",
      "        raise RuntimeError(\n",
      "            \"Reached a code path in Module.set_extra_state() that should never be called. \"\n",
      "            \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n",
      "            \"to report this bug.\")\n",
      "\n",
      "    def _apply(self, fn, recurse=True):\n",
      "        if recurse:\n",
      "            for module in self.children():\n",
      "                module._apply(fn)\n",
      "\n",
      "        def compute_should_use_set_data(tensor, tensor_applied):\n",
      "            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
      "                # If the new tensor has compatible tensor type as the existing tensor,\n",
      "                # the current behavior is to change the tensor in-place using `.data =`,\n",
      "                # and the future behavior is to overwrite the existing tensor. However,\n",
      "                # changing the current behavior is a BC-breaking change, and we want it\n",
      "                # to happen in future releases. So for now we introduce the\n",
      "                # `torch.__future__.get_overwrite_module_params_on_conversion()`\n",
      "                # global flag to let the user control whether they want the future\n",
      "                # behavior of overwriting the existing tensor or not.\n",
      "                return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
      "            else:\n",
      "                return False\n",
      "\n",
      "        for key, param in self._parameters.items():\n",
      "            if param is None:\n",
      "                continue\n",
      "            # Tensors stored in modules are graph leaves, and we don't want to\n",
      "            # track autograd history of `param_applied`, so we have to use\n",
      "            # `with torch.no_grad():`\n",
      "            with torch.no_grad():\n",
      "                param_applied = fn(param)\n",
      "            should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
      "            if should_use_set_data:\n",
      "                param.data = param_applied\n",
      "                out_param = param\n",
      "            else:\n",
      "                assert isinstance(param, Parameter)\n",
      "                assert param.is_leaf\n",
      "                out_param = Parameter(param_applied, param.requires_grad)\n",
      "                self._parameters[key] = out_param\n",
      "\n",
      "            if param.grad is not None:\n",
      "                with torch.no_grad():\n",
      "                    grad_applied = fn(param.grad)\n",
      "                should_use_set_data = compute_should_use_set_data(param.grad, grad_applied)\n",
      "                if should_use_set_data:\n",
      "                    assert out_param.grad is not None\n",
      "                    out_param.grad.data = grad_applied\n",
      "                else:\n",
      "                    assert param.grad.is_leaf\n",
      "                    out_param.grad = grad_applied.requires_grad_(param.grad.requires_grad)\n",
      "\n",
      "        for key, buf in self._buffers.items():\n",
      "            if buf is not None:\n",
      "                self._buffers[key] = fn(buf)\n",
      "\n",
      "        return self\n",
      "\n",
      "    def apply(self: T, fn: Callable[['Module'], None]) -> T:\n",
      "        r\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "        as well as self. Typical use includes initializing the parameters of a model\n",
      "        (see also :ref:`nn-init-doc`).\n",
      "\n",
      "        Args:\n",
      "            fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "\n",
      "        Example::\n",
      "\n",
      "            >>> @torch.no_grad()\n",
      "            >>> def init_weights(m):\n",
      "            >>>     print(m)\n",
      "            >>>     if type(m) == nn.Linear:\n",
      "            >>>         m.weight.fill_(1.0)\n",
      "            >>>         print(m.weight)\n",
      "            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "            >>> net.apply(init_weights)\n",
      "            Linear(in_features=2, out_features=2, bias=True)\n",
      "            Parameter containing:\n",
      "            tensor([[1., 1.],\n",
      "                    [1., 1.]], requires_grad=True)\n",
      "            Linear(in_features=2, out_features=2, bias=True)\n",
      "            Parameter containing:\n",
      "            tensor([[1., 1.],\n",
      "                    [1., 1.]], requires_grad=True)\n",
      "            Sequential(\n",
      "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "            )\n",
      "\n",
      "        \"\"\"\n",
      "        for module in self.children():\n",
      "            module.apply(fn)\n",
      "        fn(self)\n",
      "        return self\n",
      "\n",
      "    def cuda(self: T, device: Optional[Union[int, device]] = None) -> T:\n",
      "        r\"\"\"Moves all model parameters and buffers to the GPU.\n",
      "\n",
      "        This also makes associated parameters and buffers different objects. So\n",
      "        it should be called before constructing optimizer if the module will\n",
      "        live on GPU while being optimized.\n",
      "\n",
      "        .. note::\n",
      "            This method modifies the module in-place.\n",
      "\n",
      "        Args:\n",
      "            device (int, optional): if specified, all parameters will be\n",
      "                copied to that device\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self._apply(lambda t: t.cuda(device))\n",
      "\n",
      "    def ipu(self: T, device: Optional[Union[int, device]] = None) -> T:\n",
      "        r\"\"\"Moves all model parameters and buffers to the IPU.\n",
      "\n",
      "        This also makes associated parameters and buffers different objects. So\n",
      "        it should be called before constructing optimizer if the module will\n",
      "        live on IPU while being optimized.\n",
      "\n",
      "        .. note::\n",
      "            This method modifies the module in-place.\n",
      "\n",
      "        Arguments:\n",
      "            device (int, optional): if specified, all parameters will be\n",
      "                copied to that device\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self._apply(lambda t: t.ipu(device))\n",
      "\n",
      "    def xpu(self: T, device: Optional[Union[int, device]] = None) -> T:\n",
      "        r\"\"\"Moves all model parameters and buffers to the XPU.\n",
      "\n",
      "        This also makes associated parameters and buffers different objects. So\n",
      "        it should be called before constructing optimizer if the module will\n",
      "        live on XPU while being optimized.\n",
      "\n",
      "        .. note::\n",
      "            This method modifies the module in-place.\n",
      "\n",
      "        Arguments:\n",
      "            device (int, optional): if specified, all parameters will be\n",
      "                copied to that device\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self._apply(lambda t: t.xpu(device))\n",
      "\n",
      "    def cpu(self: T) -> T:\n",
      "        r\"\"\"Moves all model parameters and buffers to the CPU.\n",
      "\n",
      "        .. note::\n",
      "            This method modifies the module in-place.\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self._apply(lambda t: t.cpu())\n",
      "\n",
      "    def type(self: T, dst_type: Union[dtype, str]) -> T:\n",
      "        r\"\"\"Casts all parameters and buffers to :attr:`dst_type`.\n",
      "\n",
      "        .. note::\n",
      "            This method modifies the module in-place.\n",
      "\n",
      "        Args:\n",
      "            dst_type (type or string): the desired type\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self._apply(lambda t: t.type(dst_type))\n",
      "\n",
      "    def float(self: T) -> T:\n",
      "        r\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "\n",
      "        .. note::\n",
      "            This method modifies the module in-place.\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self._apply(lambda t: t.float() if t.is_floating_point() else t)\n",
      "\n",
      "    def double(self: T) -> T:\n",
      "        r\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "\n",
      "        .. note::\n",
      "            This method modifies the module in-place.\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self._apply(lambda t: t.double() if t.is_floating_point() else t)\n",
      "\n",
      "    def half(self: T) -> T:\n",
      "        r\"\"\"Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "\n",
      "        .. note::\n",
      "            This method modifies the module in-place.\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self._apply(lambda t: t.half() if t.is_floating_point() else t)\n",
      "\n",
      "    def bfloat16(self: T) -> T:\n",
      "        r\"\"\"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "\n",
      "        .. note::\n",
      "            This method modifies the module in-place.\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self._apply(lambda t: t.bfloat16() if t.is_floating_point() else t)\n",
      "\n",
      "    def to_empty(self: T, *, device: Union[str, device], recurse: bool = True) -> T:\n",
      "        r\"\"\"Moves the parameters and buffers to the specified device without copying storage.\n",
      "\n",
      "        Args:\n",
      "            device (:class:`torch.device`): The desired device of the parameters\n",
      "                and buffers in this module.\n",
      "            recurse (bool): Whether parameters and buffers of submodules should\n",
      "                be recursively moved to the specified device.\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self._apply(lambda t: torch.empty_like(t, device=device), recurse=recurse)\n",
      "\n",
      "    @overload\n",
      "    def to(self: T, device: Optional[Union[int, device]] = ..., dtype: Optional[Union[dtype, str]] = ...,\n",
      "           non_blocking: bool = ...) -> T:\n",
      "        ...\n",
      "\n",
      "    @overload\n",
      "    def to(self: T, dtype: Union[dtype, str], non_blocking: bool = ...) -> T:\n",
      "        ...\n",
      "\n",
      "    @overload\n",
      "    def to(self: T, tensor: Tensor, non_blocking: bool = ...) -> T:\n",
      "        ...\n",
      "\n",
      "    def to(self, *args, **kwargs):\n",
      "        r\"\"\"Moves and/or casts the parameters and buffers.\n",
      "\n",
      "        This can be called as\n",
      "\n",
      "        .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "           :noindex:\n",
      "\n",
      "        .. function:: to(dtype, non_blocking=False)\n",
      "           :noindex:\n",
      "\n",
      "        .. function:: to(tensor, non_blocking=False)\n",
      "           :noindex:\n",
      "\n",
      "        .. function:: to(memory_format=torch.channels_last)\n",
      "           :noindex:\n",
      "\n",
      "        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "        floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "        only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "        (if given). The integral parameters and buffers will be moved\n",
      "        :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "        :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "        with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "        pinned memory to CUDA devices.\n",
      "\n",
      "        See below for examples.\n",
      "\n",
      "        .. note::\n",
      "            This method modifies the module in-place.\n",
      "\n",
      "        Args:\n",
      "            device (:class:`torch.device`): the desired device of the parameters\n",
      "                and buffers in this module\n",
      "            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "                the parameters and buffers in this module\n",
      "            tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "                dtype and device for all parameters and buffers in this module\n",
      "            memory_format (:class:`torch.memory_format`): the desired memory\n",
      "                format for 4D parameters and buffers in this module (keyword\n",
      "                only argument)\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "\n",
      "        Examples::\n",
      "\n",
      "            >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "            >>> linear = nn.Linear(2, 2)\n",
      "            >>> linear.weight\n",
      "            Parameter containing:\n",
      "            tensor([[ 0.1913, -0.3420],\n",
      "                    [-0.5113, -0.2325]])\n",
      "            >>> linear.to(torch.double)\n",
      "            Linear(in_features=2, out_features=2, bias=True)\n",
      "            >>> linear.weight\n",
      "            Parameter containing:\n",
      "            tensor([[ 0.1913, -0.3420],\n",
      "                    [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      "            >>> gpu1 = torch.device(\"cuda:1\")\n",
      "            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "            Linear(in_features=2, out_features=2, bias=True)\n",
      "            >>> linear.weight\n",
      "            Parameter containing:\n",
      "            tensor([[ 0.1914, -0.3420],\n",
      "                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "            >>> cpu = torch.device(\"cpu\")\n",
      "            >>> linear.to(cpu)\n",
      "            Linear(in_features=2, out_features=2, bias=True)\n",
      "            >>> linear.weight\n",
      "            Parameter containing:\n",
      "            tensor([[ 0.1914, -0.3420],\n",
      "                    [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "\n",
      "            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "            >>> linear.weight\n",
      "            Parameter containing:\n",
      "            tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "            tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "                    [0.6122+0.j, 0.1150+0.j],\n",
      "                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)\n",
      "\n",
      "        if dtype is not None:\n",
      "            if not (dtype.is_floating_point or dtype.is_complex):\n",
      "                raise TypeError('nn.Module.to only accepts floating point or complex '\n",
      "                                f'dtypes, but got desired dtype={dtype}')\n",
      "            if dtype.is_complex:\n",
      "                warnings.warn(\n",
      "                    \"Complex modules are a new feature under active development whose design may change, \"\n",
      "                    \"and some modules might not work as expected when using complex tensors as parameters or buffers. \"\n",
      "                    \"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\n",
      "                    \"if a complex module does not work as expected.\")\n",
      "\n",
      "        def convert(t):\n",
      "            if convert_to_format is not None and t.dim() in (4, 5):\n",
      "                return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "                            non_blocking, memory_format=convert_to_format)\n",
      "            return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "\n",
      "        return self._apply(convert)\n",
      "\n",
      "    def register_full_backward_pre_hook(\n",
      "        self,\n",
      "        hook: Callable[[\"Module\", _grad_t], Union[None, _grad_t]],\n",
      "        prepend: bool = False,\n",
      "    ) -> RemovableHandle:\n",
      "        r\"\"\"Registers a backward pre-hook on the module.\n",
      "\n",
      "        The hook will be called every time the gradients for the module are computed.\n",
      "        The hook should have the following signature::\n",
      "\n",
      "            hook(module, grad_output) -> tuple[Tensor] or None\n",
      "\n",
      "        The :attr:`grad_output` is a tuple. The hook should\n",
      "        not modify its arguments, but it can optionally return a new gradient with\n",
      "        respect to the output that will be used in place of :attr:`grad_output` in\n",
      "        subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      "        all non-Tensor arguments.\n",
      "\n",
      "        For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "        of each Tensor returned by the Module's forward function.\n",
      "\n",
      "        .. warning ::\n",
      "            Modifying inputs inplace is not allowed when using backward hooks and\n",
      "            will raise an error.\n",
      "\n",
      "        Args:\n",
      "            hook (Callable): The user-defined hook to be registered.\n",
      "            prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "                all existing ``backward_pre`` hooks on this\n",
      "                :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "                ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      "                on this :class:`torch.nn.modules.Module`. Note that global\n",
      "                ``backward_pre`` hooks registered with\n",
      "                :func:`register_module_full_backward_pre_hook` will fire before\n",
      "                all hooks registered by this method.\n",
      "\n",
      "        Returns:\n",
      "            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "                a handle that can be used to remove the added hook by calling\n",
      "                ``handle.remove()``\n",
      "\n",
      "        \"\"\"\n",
      "        handle = hooks.RemovableHandle(self._backward_pre_hooks)\n",
      "        self._backward_pre_hooks[handle.id] = hook\n",
      "        if prepend:\n",
      "            self._backward_pre_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "        return handle\n",
      "\n",
      "    def register_backward_hook(\n",
      "        self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, _grad_t]]\n",
      "    ) -> RemovableHandle:\n",
      "        r\"\"\"Registers a backward hook on the module.\n",
      "\n",
      "        This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "        the behavior of this function will change in future versions.\n",
      "\n",
      "        Returns:\n",
      "            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "                a handle that can be used to remove the added hook by calling\n",
      "                ``handle.remove()``\n",
      "\n",
      "        \"\"\"\n",
      "        if self._is_full_backward_hook is True:\n",
      "            raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks on a \"\n",
      "                               \"single Module. Please use only one of them.\")\n",
      "\n",
      "        self._is_full_backward_hook = False\n",
      "\n",
      "        handle = hooks.RemovableHandle(self._backward_hooks)\n",
      "        self._backward_hooks[handle.id] = hook\n",
      "        return handle\n",
      "\n",
      "    def register_full_backward_hook(\n",
      "        self,\n",
      "        hook: Callable[[\"Module\", _grad_t, _grad_t], Union[None, _grad_t]],\n",
      "        prepend: bool = False,\n",
      "    ) -> RemovableHandle:\n",
      "        r\"\"\"Registers a backward hook on the module.\n",
      "\n",
      "        The hook will be called every time the gradients with respect to a module\n",
      "        are computed, i.e. the hook will execute if and only if the gradients with\n",
      "        respect to module outputs are computed. The hook should have the following\n",
      "        signature::\n",
      "\n",
      "            hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "\n",
      "        The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "        with respect to the inputs and outputs respectively. The hook should\n",
      "        not modify its arguments, but it can optionally return a new gradient with\n",
      "        respect to the input that will be used in place of :attr:`grad_input` in\n",
      "        subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "        as positional arguments and all kwarg arguments are ignored. Entries\n",
      "        in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "        arguments.\n",
      "\n",
      "        For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "        receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "        of each Tensor returned by the Module's forward function.\n",
      "\n",
      "        .. warning ::\n",
      "            Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "            will raise an error.\n",
      "\n",
      "        Args:\n",
      "            hook (Callable): The user-defined hook to be registered.\n",
      "            prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "                all existing ``backward`` hooks on this\n",
      "                :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "                ``hook`` will be fired after all existing ``backward`` hooks on\n",
      "                this :class:`torch.nn.modules.Module`. Note that global\n",
      "                ``backward`` hooks registered with\n",
      "                :func:`register_module_full_backward_hook` will fire before\n",
      "                all hooks registered by this method.\n",
      "\n",
      "        Returns:\n",
      "            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "                a handle that can be used to remove the added hook by calling\n",
      "                ``handle.remove()``\n",
      "\n",
      "        \"\"\"\n",
      "        if self._is_full_backward_hook is False:\n",
      "            raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks on a \"\n",
      "                               \"single Module. Please use only one of them.\")\n",
      "\n",
      "        self._is_full_backward_hook = True\n",
      "\n",
      "        handle = hooks.RemovableHandle(self._backward_hooks)\n",
      "        self._backward_hooks[handle.id] = hook\n",
      "        if prepend:\n",
      "            self._backward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "        return handle\n",
      "\n",
      "    def _get_backward_hooks(self):\n",
      "        r\"\"\"Returns the backward hooks for use in the call function.\n",
      "        It returns two lists, one with the full backward hooks and one with the non-full\n",
      "        backward hooks.\n",
      "        \"\"\"\n",
      "        full_backward_hooks: List[Callable] = []\n",
      "        if (_global_is_full_backward_hook is True):\n",
      "            full_backward_hooks += _global_backward_hooks.values()\n",
      "        if (self._is_full_backward_hook is True):\n",
      "            full_backward_hooks += self._backward_hooks.values()\n",
      "\n",
      "        non_full_backward_hooks: List[Callable] = []\n",
      "        if (_global_is_full_backward_hook is False):\n",
      "            non_full_backward_hooks += _global_backward_hooks.values()\n",
      "        if (self._is_full_backward_hook is False):\n",
      "            non_full_backward_hooks += self._backward_hooks.values()\n",
      "\n",
      "        return full_backward_hooks, non_full_backward_hooks\n",
      "\n",
      "    def _get_backward_pre_hooks(self):\n",
      "        backward_pre_hooks: List[Callable] = []\n",
      "        backward_pre_hooks += _global_backward_pre_hooks.values()\n",
      "        backward_pre_hooks += self._backward_pre_hooks.values()\n",
      "\n",
      "        return backward_pre_hooks\n",
      "\n",
      "    def _maybe_warn_non_full_backward_hook(self, inputs, result, grad_fn):\n",
      "        if not isinstance(result, torch.Tensor):\n",
      "            if not (isinstance(result, tuple) and all(isinstance(r, torch.Tensor) for r in result)):\n",
      "                warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "                              \"single Tensor or a tuple of Tensors is deprecated and will be removed \"\n",
      "                              \"in future versions. This hook will be missing some of the grad_output. \"\n",
      "                              \"Please use register_full_backward_hook to get the documented behavior.\")\n",
      "                return\n",
      "        else:\n",
      "            result = (result,)\n",
      "\n",
      "        if not isinstance(inputs, torch.Tensor):\n",
      "            if not (isinstance(inputs, tuple) and all(isinstance(i, torch.Tensor) for i in inputs)):\n",
      "                warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "                              \"single Tensor or a tuple of Tensors is deprecated and will be removed \"\n",
      "                              \"in future versions. This hook will be missing some of the grad_input. \"\n",
      "                              \"Please use register_full_backward_hook to get the documented behavior.\")\n",
      "                return\n",
      "        else:\n",
      "            inputs = (inputs,)\n",
      "\n",
      "        # At this point we are sure that inputs and result are tuple of Tensors\n",
      "        out_grad_fn = {r.grad_fn for r in result if r.grad_fn is not None}\n",
      "        if len(out_grad_fn) == 0 or (len(out_grad_fn) == 1 and grad_fn not in out_grad_fn):\n",
      "            warnings.warn(\"Using a non-full backward hook when outputs are nested in python data structure \"\n",
      "                          \"is deprecated and will be removed in future versions. This hook will be missing \"\n",
      "                          \"some grad_output.\")\n",
      "        elif len(out_grad_fn) > 1:\n",
      "            warnings.warn(\"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\n",
      "                          \"is deprecated and will be removed in future versions. This hook will be missing \"\n",
      "                          \"some grad_output. Please use register_full_backward_hook to get the documented behavior.\")\n",
      "        else:\n",
      "            # At this point the grad_output part of the hook will most likely be correct\n",
      "            inputs_grad_fn = {i.grad_fn for i in inputs if i.grad_fn is not None}\n",
      "\n",
      "            next_functions = {n[0] for n in grad_fn.next_functions}\n",
      "\n",
      "            if inputs_grad_fn != next_functions:\n",
      "                warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "                              \"is deprecated and will be removed in future versions. This hook will be missing \"\n",
      "                              \"some grad_input. Please use register_full_backward_hook to get the documented \"\n",
      "                              \"behavior.\")\n",
      "\n",
      "    def register_forward_pre_hook(\n",
      "        self,\n",
      "        hook: Union[\n",
      "            Callable[[T, Tuple[Any, ...]], Optional[Any]],\n",
      "            Callable[[T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]],\n",
      "        ],\n",
      "        *,\n",
      "        prepend: bool = False,\n",
      "        with_kwargs: bool = False,\n",
      "    ) -> RemovableHandle:\n",
      "        r\"\"\"Registers a forward pre-hook on the module.\n",
      "\n",
      "        The hook will be called every time before :func:`forward` is invoked.\n",
      "\n",
      "\n",
      "        If ``with_kwargs`` is false or not specified, the input contains only\n",
      "        the positional arguments given to the module. Keyword arguments won't be\n",
      "        passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "        input. User can either return a tuple or a single modified value in the\n",
      "        hook. We will wrap the value into a tuple if a single value is returned\n",
      "        (unless that value is already a tuple). The hook should have the\n",
      "        following signature::\n",
      "\n",
      "            hook(module, args) -> None or modified input\n",
      "\n",
      "        If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      "        kwargs given to the forward function. And if the hook modifies the\n",
      "        input, both the args and kwargs should be returned. The hook should have\n",
      "        the following signature::\n",
      "\n",
      "            hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      "\n",
      "        Args:\n",
      "            hook (Callable): The user defined hook to be registered.\n",
      "            prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "                all existing ``forward_pre`` hooks on this\n",
      "                :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "                ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      "                on this :class:`torch.nn.modules.Module`. Note that global\n",
      "                ``forward_pre`` hooks registered with\n",
      "                :func:`register_module_forward_pre_hook` will fire before all\n",
      "                hooks registered by this method.\n",
      "                Default: ``False``\n",
      "            with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      "                given to the forward function.\n",
      "                Default: ``False``\n",
      "\n",
      "        Returns:\n",
      "            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "                a handle that can be used to remove the added hook by calling\n",
      "                ``handle.remove()``\n",
      "        \"\"\"\n",
      "        handle = hooks.RemovableHandle(\n",
      "            self._forward_pre_hooks,\n",
      "            extra_dict=self._forward_pre_hooks_with_kwargs\n",
      "        )\n",
      "        self._forward_pre_hooks[handle.id] = hook\n",
      "        if with_kwargs:\n",
      "            self._forward_pre_hooks_with_kwargs[handle.id] = True\n",
      "\n",
      "        if prepend:\n",
      "            self._forward_pre_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "        return handle\n",
      "\n",
      "    def register_forward_hook(\n",
      "        self,\n",
      "        hook: Union[\n",
      "            Callable[[T, Tuple[Any, ...], Any], Optional[Any]],\n",
      "            Callable[[T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]],\n",
      "        ],\n",
      "        *,\n",
      "        prepend: bool = False,\n",
      "        with_kwargs: bool = False,\n",
      "        always_call: bool = False,\n",
      "    ) -> RemovableHandle:\n",
      "        r\"\"\"Registers a forward hook on the module.\n",
      "\n",
      "        The hook will be called every time after :func:`forward` has computed an output.\n",
      "\n",
      "        If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      "        the positional arguments given to the module. Keyword arguments won't be\n",
      "        passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "        output. It can modify the input inplace but it will not have effect on\n",
      "        forward since this is called after :func:`forward` is called. The hook\n",
      "        should have the following signature::\n",
      "\n",
      "            hook(module, args, output) -> None or modified output\n",
      "\n",
      "        If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      "        ``kwargs`` given to the forward function and be expected to return the\n",
      "        output possibly modified. The hook should have the following signature::\n",
      "\n",
      "            hook(module, args, kwargs, output) -> None or modified output\n",
      "\n",
      "        Args:\n",
      "            hook (Callable): The user defined hook to be registered.\n",
      "            prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      "                before all existing ``forward`` hooks on this\n",
      "                :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "                ``hook`` will be fired after all existing ``forward`` hooks on\n",
      "                this :class:`torch.nn.modules.Module`. Note that global\n",
      "                ``forward`` hooks registered with\n",
      "                :func:`register_module_forward_hook` will fire before all hooks\n",
      "                registered by this method.\n",
      "                Default: ``False``\n",
      "            with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      "                kwargs given to the forward function.\n",
      "                Default: ``False``\n",
      "            always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      "                whether an exception is raised while calling the Module.\n",
      "                Default: ``False``\n",
      "\n",
      "        Returns:\n",
      "            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "                a handle that can be used to remove the added hook by calling\n",
      "                ``handle.remove()``\n",
      "        \"\"\"\n",
      "        handle = hooks.RemovableHandle(\n",
      "            self._forward_hooks,\n",
      "            extra_dict=[self._forward_hooks_with_kwargs, self._forward_hooks_always_called],\n",
      "        )\n",
      "        self._forward_hooks[handle.id] = hook\n",
      "        if with_kwargs:\n",
      "            self._forward_hooks_with_kwargs[handle.id] = True\n",
      "        if always_call:\n",
      "            self._forward_hooks_always_called[handle.id] = True\n",
      "        if prepend:\n",
      "            self._forward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "        return handle\n",
      "\n",
      "    def _slow_forward(self, *input, **kwargs):\n",
      "        tracing_state = torch._C._get_tracing_state()\n",
      "        if not tracing_state or isinstance(self.forward, torch._C.ScriptMethod):\n",
      "            return self.forward(*input, **kwargs)\n",
      "        recording_scopes = torch.jit._trace._trace_module_map is not None\n",
      "        if recording_scopes:\n",
      "            # type ignore was added because at this point one knows that\n",
      "            # torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\n",
      "            name = torch.jit._trace._trace_module_map[self] if self in torch.jit._trace._trace_module_map else None  # type: ignore[index, operator] # noqa: B950\n",
      "            if name:\n",
      "                tracing_state.push_scope(name)\n",
      "            else:\n",
      "                recording_scopes = False\n",
      "        try:\n",
      "            result = self.forward(*input, **kwargs)\n",
      "        finally:\n",
      "            if recording_scopes:\n",
      "                tracing_state.pop_scope()\n",
      "        return result\n",
      "\n",
      "    def _wrapped_call_impl(self, *args, **kwargs):\n",
      "        if self._compiled_call_impl is not None:\n",
      "            return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
      "        else:\n",
      "            return self._call_impl(*args, **kwargs)\n",
      "\n",
      "    def _call_impl(self, *args, **kwargs):\n",
      "        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n",
      "        # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "        # this function, and just call forward.\n",
      "        if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
      "                or _global_backward_pre_hooks or _global_backward_hooks\n",
      "                or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "            return forward_call(*args, **kwargs)\n",
      "\n",
      "        try:\n",
      "            result = None\n",
      "            called_always_called_hooks = set()\n",
      "\n",
      "            full_backward_hooks, non_full_backward_hooks = [], []\n",
      "            backward_pre_hooks = []\n",
      "            if self._backward_pre_hooks or _global_backward_pre_hooks:\n",
      "                backward_pre_hooks = self._get_backward_pre_hooks()\n",
      "\n",
      "            if self._backward_hooks or _global_backward_hooks:\n",
      "                full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()\n",
      "\n",
      "            if _global_forward_pre_hooks or self._forward_pre_hooks:\n",
      "                for hook_id, hook in (\n",
      "                    *_global_forward_pre_hooks.items(),\n",
      "                    *self._forward_pre_hooks.items(),\n",
      "                ):\n",
      "                    if hook_id in self._forward_pre_hooks_with_kwargs:\n",
      "                        args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]\n",
      "                        if args_kwargs_result is not None:\n",
      "                            if isinstance(args_kwargs_result, tuple) and len(args_kwargs_result) == 2:\n",
      "                                args, kwargs = args_kwargs_result\n",
      "                            else:\n",
      "                                raise RuntimeError(\n",
      "                                    \"forward pre-hook must return None or a tuple \"\n",
      "                                    f\"of (new_args, new_kwargs), but got {args_kwargs_result}.\"\n",
      "                                )\n",
      "                    else:\n",
      "                        args_result = hook(self, args)\n",
      "                        if args_result is not None:\n",
      "                            if not isinstance(args_result, tuple):\n",
      "                                args_result = (args_result,)\n",
      "                            args = args_result\n",
      "\n",
      "            bw_hook = None\n",
      "            if full_backward_hooks or backward_pre_hooks:\n",
      "                bw_hook = hooks.BackwardHook(self, full_backward_hooks, backward_pre_hooks)\n",
      "                args = bw_hook.setup_input_hook(args)\n",
      "\n",
      "            result = forward_call(*args, **kwargs)\n",
      "            if _global_forward_hooks or self._forward_hooks:\n",
      "                for hook_id, hook in (\n",
      "                    *_global_forward_hooks.items(),\n",
      "                    *self._forward_hooks.items(),\n",
      "                ):\n",
      "                    # mark that always called hook is run\n",
      "                    if hook_id in self._forward_hooks_always_called or hook_id in _global_forward_hooks_always_called:\n",
      "                        called_always_called_hooks.add(hook_id)\n",
      "\n",
      "                    if hook_id in self._forward_hooks_with_kwargs:\n",
      "                        hook_result = hook(self, args, kwargs, result)\n",
      "                    else:\n",
      "                        hook_result = hook(self, args, result)\n",
      "\n",
      "                    if hook_result is not None:\n",
      "                        result = hook_result\n",
      "\n",
      "            if bw_hook:\n",
      "                if not isinstance(result, (torch.Tensor, tuple)):\n",
      "                    warnings.warn(\"For backward hooks to be called,\"\n",
      "                                  \" module output should be a Tensor or a tuple of Tensors\"\n",
      "                                  f\" but received {type(result)}\")\n",
      "                result = bw_hook.setup_output_hook(result)\n",
      "\n",
      "            # Handle the non-full backward hooks\n",
      "            if non_full_backward_hooks:\n",
      "                var = result\n",
      "                while not isinstance(var, torch.Tensor):\n",
      "                    if isinstance(var, dict):\n",
      "                        var = next(v for v in var.values() if isinstance(v, torch.Tensor))\n",
      "                    else:\n",
      "                        var = var[0]\n",
      "                grad_fn = var.grad_fn\n",
      "                if grad_fn is not None:\n",
      "                    for hook in non_full_backward_hooks:\n",
      "                        grad_fn.register_hook(_WrappedHook(hook, self))\n",
      "                    self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "\n",
      "            return result\n",
      "\n",
      "        except Exception:\n",
      "            # run always called hooks if they have not already been run\n",
      "            # For now only forward hooks have the always_call option but perhaps\n",
      "            # this functionality should be added to full backward hooks as well.\n",
      "            for hook_id, hook in _global_forward_hooks.items():\n",
      "                if hook_id in _global_forward_hooks_always_called and hook_id not in called_always_called_hooks:\n",
      "                    try:\n",
      "                        hook_result = hook(self, args, result)\n",
      "                        if hook_result is not None:\n",
      "                            result = hook_result\n",
      "                    except Exception as e:\n",
      "                        warnings.warn(\"global module forward hook with ``always_call=True`` raised an exception \"\n",
      "                                      f\"that was silenced as another error was raised in forward: {str(e)}\")\n",
      "                        continue\n",
      "\n",
      "            for hook_id, hook in self._forward_hooks.items():\n",
      "                if hook_id in self._forward_hooks_always_called and hook_id not in called_always_called_hooks:\n",
      "                    try:\n",
      "                        if hook_id in self._forward_hooks_with_kwargs:\n",
      "                            hook_result = hook(self, args, kwargs, result)\n",
      "                        else:\n",
      "                            hook_result = hook(self, args, result)\n",
      "                        if hook_result is not None:\n",
      "                            result = hook_result\n",
      "                    except Exception as e:\n",
      "                        warnings.warn(\"module forward hook with ``always_call=True`` raised an exception \"\n",
      "                                      f\"that was silenced as another error was raised in forward: {str(e)}\")\n",
      "                        continue\n",
      "            # raise exception raised in try block\n",
      "            raise\n",
      "\n",
      "\n",
      "    __call__ : Callable[..., Any] = _wrapped_call_impl\n",
      "\n",
      "    def __getstate__(self):\n",
      "        state = self.__dict__.copy()\n",
      "        state.pop(\"_compiled_call_impl\", None)\n",
      "        return state\n",
      "\n",
      "    def __setstate__(self, state):\n",
      "        self.__dict__.update(state)\n",
      "\n",
      "        # Support loading old checkpoints that don't have the following attrs:\n",
      "        if '_forward_pre_hooks' not in self.__dict__:\n",
      "            self._forward_pre_hooks = OrderedDict()\n",
      "        if '_forward_pre_hooks_with_kwargs' not in self.__dict__:\n",
      "            self._forward_pre_hooks_with_kwargs = OrderedDict()\n",
      "        if '_forward_hooks_with_kwargs' not in self.__dict__:\n",
      "            self._forward_hooks_with_kwargs = OrderedDict()\n",
      "        if '_forward_hooks_always_called' not in self.__dict__:\n",
      "            self._forward_hooks_always_called = OrderedDict()\n",
      "        if '_state_dict_hooks' not in self.__dict__:\n",
      "            self._state_dict_hooks = OrderedDict()\n",
      "        if '_state_dict_pre_hooks' not in self.__dict__:\n",
      "            self._state_dict_pre_hooks = OrderedDict()\n",
      "        if '_load_state_dict_pre_hooks' not in self.__dict__:\n",
      "            self._load_state_dict_pre_hooks = OrderedDict()\n",
      "        if '_load_state_dict_post_hooks' not in self.__dict__:\n",
      "            self._load_state_dict_post_hooks = OrderedDict()\n",
      "        if '_non_persistent_buffers_set' not in self.__dict__:\n",
      "            self._non_persistent_buffers_set = set()\n",
      "        if '_is_full_backward_hook' not in self.__dict__:\n",
      "            self._is_full_backward_hook = None\n",
      "        if '_backward_pre_hooks' not in self.__dict__:\n",
      "            self._backward_pre_hooks = OrderedDict()\n",
      "\n",
      "    # On the return type:\n",
      "    # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      "    # This is done for better interop with various type checkers for the end users.\n",
      "    # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      "    # people to excessively use type-ignores, asserts, casts, etc.\n",
      "    # See full discussion on the problems with returning `Union` here\n",
      "    # https://github.com/microsoft/pyright/issues/4213\n",
      "    def __getattr__(self, name: str) -> Any:\n",
      "        if '_parameters' in self.__dict__:\n",
      "            _parameters = self.__dict__['_parameters']\n",
      "            if name in _parameters:\n",
      "                return _parameters[name]\n",
      "        if '_buffers' in self.__dict__:\n",
      "            _buffers = self.__dict__['_buffers']\n",
      "            if name in _buffers:\n",
      "                return _buffers[name]\n",
      "        if '_modules' in self.__dict__:\n",
      "            modules = self.__dict__['_modules']\n",
      "            if name in modules:\n",
      "                return modules[name]\n",
      "        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "\n",
      "    def __setattr__(self, name: str, value: Union[Tensor, 'Module']) -> None:\n",
      "        def remove_from(*dicts_or_sets):\n",
      "            for d in dicts_or_sets:\n",
      "                if name in d:\n",
      "                    if isinstance(d, dict):\n",
      "                        del d[name]\n",
      "                    else:\n",
      "                        d.discard(name)\n",
      "\n",
      "        params = self.__dict__.get('_parameters')\n",
      "        if isinstance(value, Parameter):\n",
      "            if params is None:\n",
      "                raise AttributeError(\n",
      "                    \"cannot assign parameters before Module.__init__() call\")\n",
      "            remove_from(self.__dict__, self._buffers, self._modules, self._non_persistent_buffers_set)\n",
      "            self.register_parameter(name, value)\n",
      "        elif params is not None and name in params:\n",
      "            if value is not None:\n",
      "                raise TypeError(f\"cannot assign '{torch.typename(value)}' as parameter '{name}' \"\n",
      "                                \"(torch.nn.Parameter or None expected)\"\n",
      "                                )\n",
      "            self.register_parameter(name, value)\n",
      "        else:\n",
      "            modules = self.__dict__.get('_modules')\n",
      "            if isinstance(value, Module):\n",
      "                if modules is None:\n",
      "                    raise AttributeError(\n",
      "                        \"cannot assign module before Module.__init__() call\")\n",
      "                remove_from(self.__dict__, self._parameters, self._buffers, self._non_persistent_buffers_set)\n",
      "                for hook in _global_module_registration_hooks.values():\n",
      "                    output = hook(self, name, value)\n",
      "                    if output is not None:\n",
      "                        value = output\n",
      "                modules[name] = value\n",
      "            elif modules is not None and name in modules:\n",
      "                if value is not None:\n",
      "                    raise TypeError(f\"cannot assign '{torch.typename(value)}' as child module '{name}' \"\n",
      "                                    \"(torch.nn.Module or None expected)\"\n",
      "                                    )\n",
      "                for hook in _global_module_registration_hooks.values():\n",
      "                    output = hook(self, name, value)\n",
      "                    if output is not None:\n",
      "                        value = output\n",
      "                modules[name] = value\n",
      "            else:\n",
      "                buffers = self.__dict__.get('_buffers')\n",
      "                if buffers is not None and name in buffers:\n",
      "                    if value is not None and not isinstance(value, torch.Tensor):\n",
      "                        raise TypeError(f\"cannot assign '{torch.typename(value)}' as buffer '{name}' \"\n",
      "                                        \"(torch.Tensor or None expected)\"\n",
      "                                        )\n",
      "                    for hook in _global_buffer_registration_hooks.values():\n",
      "                        output = hook(self, name, value)\n",
      "                        if output is not None:\n",
      "                            value = output\n",
      "                    buffers[name] = value\n",
      "                else:\n",
      "                    super().__setattr__(name, value)\n",
      "\n",
      "    def __delattr__(self, name):\n",
      "        if name in self._parameters:\n",
      "            del self._parameters[name]\n",
      "        elif name in self._buffers:\n",
      "            del self._buffers[name]\n",
      "            self._non_persistent_buffers_set.discard(name)\n",
      "        elif name in self._modules:\n",
      "            del self._modules[name]\n",
      "        else:\n",
      "            super().__delattr__(name)\n",
      "\n",
      "    def _register_state_dict_hook(self, hook):\n",
      "        r\"\"\"These hooks will be called with arguments: `self`, `state_dict`,\n",
      "        `prefix`, `local_metadata`, after the `state_dict` of `self` is set.\n",
      "        Note that only parameters and buffers of `self` or its children are\n",
      "        guaranteed to exist in `state_dict`. The hooks may modify `state_dict`\n",
      "        inplace or return a new one.\n",
      "        \"\"\"\n",
      "        handle = hooks.RemovableHandle(self._state_dict_hooks)\n",
      "        self._state_dict_hooks[handle.id] = hook\n",
      "        return handle\n",
      "\n",
      "    def register_state_dict_pre_hook(self, hook):\n",
      "        r\"\"\"These hooks will be called with arguments: ``self``, ``prefix``,\n",
      "        and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      "        hooks can be used to perform pre-processing before the ``state_dict``\n",
      "        call is made.\n",
      "        \"\"\"\n",
      "        handle = hooks.RemovableHandle(self._state_dict_pre_hooks)\n",
      "        self._state_dict_pre_hooks[handle.id] = hook\n",
      "        return handle\n",
      "\n",
      "    def _save_to_state_dict(self, destination, prefix, keep_vars):\n",
      "        r\"\"\"Saves module state to `destination` dictionary, containing a state\n",
      "        of the module, but not its descendants. This is called on every\n",
      "        submodule in :meth:`~torch.nn.Module.state_dict`.\n",
      "\n",
      "        In rare cases, subclasses can achieve class-specific behavior by\n",
      "        overriding this method with custom logic.\n",
      "\n",
      "        Args:\n",
      "            destination (dict): a dict where state will be stored\n",
      "            prefix (str): the prefix for parameters and buffers used in this\n",
      "                module\n",
      "        \"\"\"\n",
      "        for name, param in self._parameters.items():\n",
      "            if param is not None:\n",
      "                destination[prefix + name] = param if keep_vars else param.detach()\n",
      "        for name, buf in self._buffers.items():\n",
      "            if buf is not None and name not in self._non_persistent_buffers_set:\n",
      "                destination[prefix + name] = buf if keep_vars else buf.detach()\n",
      "        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n",
      "        if getattr(self.__class__, \"get_extra_state\", Module.get_extra_state) is not Module.get_extra_state:\n",
      "            destination[extra_state_key] = self.get_extra_state()\n",
      "\n",
      "    # The user can pass an optional arbitrary mappable object to `state_dict`, in which case `state_dict` returns\n",
      "    # back that same object. But if they pass nothing, an `OrderedDict` is created and returned.\n",
      "    T_destination = TypeVar('T_destination', bound=Dict[str, Any])\n",
      "\n",
      "    @overload\n",
      "    def state_dict(self, *, destination: T_destination, prefix: str = ..., keep_vars: bool = ...) -> T_destination:\n",
      "        ...\n",
      "\n",
      "    @overload\n",
      "    def state_dict(self, *, prefix: str = ..., keep_vars: bool = ...) -> Dict[str, Any]:\n",
      "        ...\n",
      "\n",
      "    # TODO: Change `*args` to `*` and remove the corresponding warning in docs when BC allows.\n",
      "    # Also remove the logic for arg parsing together.\n",
      "    def state_dict(self, *args, destination=None, prefix='', keep_vars=False):\n",
      "        r\"\"\"Returns a dictionary containing references to the whole state of the module.\n",
      "\n",
      "        Both parameters and persistent buffers (e.g. running averages) are\n",
      "        included. Keys are corresponding parameter and buffer names.\n",
      "        Parameters and buffers set to ``None`` are not included.\n",
      "\n",
      "        .. note::\n",
      "            The returned object is a shallow copy. It contains references\n",
      "            to the module's parameters and buffers.\n",
      "\n",
      "        .. warning::\n",
      "            Currently ``state_dict()`` also accepts positional arguments for\n",
      "            ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "            this is being deprecated and keyword arguments will be enforced in\n",
      "            future releases.\n",
      "\n",
      "        .. warning::\n",
      "            Please avoid the use of argument ``destination`` as it is not\n",
      "            designed for end-users.\n",
      "\n",
      "        Args:\n",
      "            destination (dict, optional): If provided, the state of module will\n",
      "                be updated into the dict and the same object is returned.\n",
      "                Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "                Default: ``None``.\n",
      "            prefix (str, optional): a prefix added to parameter and buffer\n",
      "                names to compose the keys in state_dict. Default: ``''``.\n",
      "            keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "                returned in the state dict are detached from autograd. If it's\n",
      "                set to ``True``, detaching will not be performed.\n",
      "                Default: ``False``.\n",
      "\n",
      "        Returns:\n",
      "            dict:\n",
      "                a dictionary containing a whole state of the module\n",
      "\n",
      "        Example::\n",
      "\n",
      "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "            >>> module.state_dict().keys()\n",
      "            ['bias', 'weight']\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # TODO: Remove `args` and the parsing logic when BC allows.\n",
      "        if len(args) > 0:\n",
      "            if destination is None:\n",
      "                destination = args[0]\n",
      "            if len(args) > 1 and prefix == '':\n",
      "                prefix = args[1]\n",
      "            if len(args) > 2 and keep_vars is False:\n",
      "                keep_vars = args[2]\n",
      "            # DeprecationWarning is ignored by default\n",
      "            warnings.warn(\n",
      "                \"Positional args are being deprecated, use kwargs instead. Refer to \"\n",
      "                \"https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict\"\n",
      "                \" for details.\")\n",
      "\n",
      "        if destination is None:\n",
      "            destination = OrderedDict()\n",
      "            destination._metadata = OrderedDict()\n",
      "\n",
      "        local_metadata = dict(version=self._version)\n",
      "        if hasattr(destination, \"_metadata\"):\n",
      "            destination._metadata[prefix[:-1]] = local_metadata\n",
      "\n",
      "        for hook in self._state_dict_pre_hooks.values():\n",
      "            hook(self, prefix, keep_vars)\n",
      "        self._save_to_state_dict(destination, prefix, keep_vars)\n",
      "        for name, module in self._modules.items():\n",
      "            if module is not None:\n",
      "                module.state_dict(destination=destination, prefix=prefix + name + '.', keep_vars=keep_vars)\n",
      "        for hook in self._state_dict_hooks.values():\n",
      "            hook_result = hook(self, destination, prefix, local_metadata)\n",
      "            if hook_result is not None:\n",
      "                destination = hook_result\n",
      "        return destination\n",
      "\n",
      "    def _register_load_state_dict_pre_hook(self, hook, with_module=False):\n",
      "        r\"\"\"These hooks will be called with arguments: `state_dict`, `prefix`,\n",
      "        `local_metadata`, `strict`, `missing_keys`, `unexpected_keys`,\n",
      "        `error_msgs`, before loading `state_dict` into `self`. These arguments\n",
      "        are exactly the same as those of `_load_from_state_dict`.\n",
      "\n",
      "        If ``with_module`` is ``True``, then the first argument to the hook is\n",
      "        an instance of the module.\n",
      "\n",
      "        Arguments:\n",
      "            hook (Callable): Callable hook that will be invoked before\n",
      "                loading the state dict.\n",
      "            with_module (bool, optional): Whether or not to pass the module\n",
      "                instance to the hook as the first parameter.\n",
      "        \"\"\"\n",
      "        handle = hooks.RemovableHandle(self._load_state_dict_pre_hooks)\n",
      "        self._load_state_dict_pre_hooks[handle.id] = _WrappedHook(hook, self if with_module else None)\n",
      "        return handle\n",
      "\n",
      "    def register_load_state_dict_post_hook(self, hook):\n",
      "        r\"\"\"Registers a post hook to be run after module's ``load_state_dict``\n",
      "        is called.\n",
      "\n",
      "        It should have the following signature::\n",
      "            hook(module, incompatible_keys) -> None\n",
      "\n",
      "        The ``module`` argument is the current module that this hook is registered\n",
      "        on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "        of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "        is a ``list`` of ``str`` containing the missing keys and\n",
      "        ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "\n",
      "        The given incompatible_keys can be modified inplace if needed.\n",
      "\n",
      "        Note that the checks performed when calling :func:`load_state_dict` with\n",
      "        ``strict=True`` are affected by modifications the hook makes to\n",
      "        ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "        set of keys will result in an error being thrown when ``strict=True``, and\n",
      "        clearing out both missing and unexpected keys will avoid an error.\n",
      "\n",
      "        Returns:\n",
      "            :class:`torch.utils.hooks.RemovableHandle`:\n",
      "                a handle that can be used to remove the added hook by calling\n",
      "                ``handle.remove()``\n",
      "        \"\"\"\n",
      "        handle = hooks.RemovableHandle(self._load_state_dict_post_hooks)\n",
      "        self._load_state_dict_post_hooks[handle.id] = hook\n",
      "        return handle\n",
      "\n",
      "\n",
      "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
      "                              missing_keys, unexpected_keys, error_msgs):\n",
      "        r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n",
      "        this module, but not its descendants. This is called on every submodule\n",
      "        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n",
      "        module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n",
      "        For state dicts without metadata, :attr:`local_metadata` is empty.\n",
      "        Subclasses can achieve class-specific backward compatible loading using\n",
      "        the version number at `local_metadata.get(\"version\", None)`.\n",
      "        Additionally, :attr:`local_metadata` can also contain the key\n",
      "        `assign_to_params_buffers` that indicates whether keys should be\n",
      "        assigned their corresponding tensor in the state_dict.\n",
      "\n",
      "        .. note::\n",
      "            :attr:`state_dict` is not the same object as the input\n",
      "            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n",
      "            it can be modified.\n",
      "\n",
      "        Args:\n",
      "            state_dict (dict): a dict containing parameters and\n",
      "                persistent buffers.\n",
      "            prefix (str): the prefix for parameters and buffers used in this\n",
      "                module\n",
      "            local_metadata (dict): a dict containing the metadata for this module.\n",
      "                See\n",
      "            strict (bool): whether to strictly enforce that the keys in\n",
      "                :attr:`state_dict` with :attr:`prefix` match the names of\n",
      "                parameters and buffers in this module\n",
      "            missing_keys (list of str): if ``strict=True``, add missing keys to\n",
      "                this list\n",
      "            unexpected_keys (list of str): if ``strict=True``, add unexpected\n",
      "                keys to this list\n",
      "            error_msgs (list of str): error messages should be added to this\n",
      "                list, and will be reported together in\n",
      "                :meth:`~torch.nn.Module.load_state_dict`\n",
      "        \"\"\"\n",
      "        for hook in self._load_state_dict_pre_hooks.values():\n",
      "            hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\n",
      "\n",
      "        persistent_buffers = {k: v for k, v in self._buffers.items() if k not in self._non_persistent_buffers_set}\n",
      "        local_name_params = itertools.chain(self._parameters.items(), persistent_buffers.items())\n",
      "        local_state = {k: v for k, v in local_name_params if v is not None}\n",
      "        assign_to_params_buffers = local_metadata.get(\"assign_to_params_buffers\", False)\n",
      "\n",
      "        for name, param in local_state.items():\n",
      "            key = prefix + name\n",
      "            if key in state_dict:\n",
      "                input_param = state_dict[key]\n",
      "                if not torch.overrides.is_tensor_like(input_param):\n",
      "                    error_msgs.append(f'While copying the parameter named \"{key}\", '\n",
      "                                      'expected torch.Tensor or Tensor-like object from checkpoint but '\n",
      "                                      f'received {type(input_param)}'\n",
      "                                      )\n",
      "                    continue\n",
      "\n",
      "                # This is used to avoid copying uninitialized parameters into\n",
      "                # non-lazy modules, since they dont have the hook to do the checks\n",
      "                # in such case, it will error when accessing the .shape attribute.\n",
      "                is_param_lazy = torch.nn.parameter.is_lazy(param)\n",
      "                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n",
      "                if not is_param_lazy and len(param.shape) == 0 and len(input_param.shape) == 1:\n",
      "                    input_param = input_param[0]\n",
      "\n",
      "                if not is_param_lazy and input_param.shape != param.shape:\n",
      "                    # local shape should match the one in checkpoint\n",
      "                    error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
      "                                      'the shape in current model is {}.'\n",
      "                                      .format(key, input_param.shape, param.shape))\n",
      "                    continue\n",
      "\n",
      "                if param.is_meta and not input_param.is_meta and not assign_to_params_buffers:\n",
      "                    warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "                                  'parameter in the current model, which is a no-op. (Did you mean to '\n",
      "                                  'pass `assign=True` to assign items in the state dictionary to their '\n",
      "                                  'corresponding key in the module instead of copying them in place?)')\n",
      "\n",
      "                try:\n",
      "                    with torch.no_grad():\n",
      "                        if assign_to_params_buffers:\n",
      "                            # Shape checks are already done above\n",
      "                            if (isinstance(param, torch.nn.Parameter) and\n",
      "                                    not isinstance(input_param, torch.nn.Parameter)):\n",
      "                                setattr(self, name, torch.nn.Parameter(input_param))\n",
      "                            else:\n",
      "                                setattr(self, name, input_param)\n",
      "                        else:\n",
      "                            param.copy_(input_param)\n",
      "                except Exception as ex:\n",
      "                    error_msgs.append(f'While copying the parameter named \"{key}\", '\n",
      "                                      f'whose dimensions in the model are {param.size()} and '\n",
      "                                      f'whose dimensions in the checkpoint are {input_param.size()}, '\n",
      "                                      f'an exception occurred : {ex.args}.'\n",
      "                                      )\n",
      "            elif strict:\n",
      "                missing_keys.append(key)\n",
      "\n",
      "        extra_state_key = prefix + _EXTRA_STATE_KEY_SUFFIX\n",
      "        if getattr(self.__class__, \"set_extra_state\", Module.set_extra_state) is not Module.set_extra_state:\n",
      "            if extra_state_key in state_dict:\n",
      "                self.set_extra_state(state_dict[extra_state_key])\n",
      "            elif strict:\n",
      "                missing_keys.append(extra_state_key)\n",
      "        elif strict and (extra_state_key in state_dict):\n",
      "            unexpected_keys.append(extra_state_key)\n",
      "\n",
      "        if strict:\n",
      "            for key in state_dict.keys():\n",
      "                if key.startswith(prefix) and key != extra_state_key:\n",
      "                    input_name = key[len(prefix):]\n",
      "                    input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child\n",
      "                    if input_name not in self._modules and input_name not in local_state:\n",
      "                        unexpected_keys.append(key)\n",
      "\n",
      "    def load_state_dict(self, state_dict: Mapping[str, Any],\n",
      "                        strict: bool = True, assign: bool = False):\n",
      "        r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n",
      "        this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "        the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "        by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "\n",
      "        .. warning::\n",
      "            If :attr:`assign` is ``True`` the optimizer must be created after\n",
      "            the call to :attr:`load_state_dict`.\n",
      "\n",
      "        Args:\n",
      "            state_dict (dict): a dict containing parameters and\n",
      "                persistent buffers.\n",
      "            strict (bool, optional): whether to strictly enforce that the keys\n",
      "                in :attr:`state_dict` match the keys returned by this module's\n",
      "                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "            assign (bool, optional): whether to assign items in the state\n",
      "                dictionary to their corresponding keys in the module instead\n",
      "                of copying them inplace into the module's current parameters and buffers.\n",
      "                When ``False``, the properties of the tensors in the current\n",
      "                module are preserved while when ``True``, the properties of the\n",
      "                Tensors in the state dict are preserved.\n",
      "                Default: ``False``\n",
      "\n",
      "        Returns:\n",
      "            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "                * **missing_keys** is a list of str containing the missing keys\n",
      "                * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "\n",
      "        Note:\n",
      "            If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "            exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "            ``RuntimeError``.\n",
      "        \"\"\"\n",
      "        if not isinstance(state_dict, Mapping):\n",
      "            raise TypeError(f\"Expected state_dict to be dict-like, got {type(state_dict)}.\")\n",
      "\n",
      "        missing_keys: List[str] = []\n",
      "        unexpected_keys: List[str] = []\n",
      "        error_msgs: List[str] = []\n",
      "\n",
      "        # copy state_dict so _load_from_state_dict can modify it\n",
      "        metadata = getattr(state_dict, '_metadata', None)\n",
      "        state_dict = OrderedDict(state_dict)\n",
      "        if metadata is not None:\n",
      "            # mypy isn't aware that \"_metadata\" exists in state_dict\n",
      "            state_dict._metadata = metadata  # type: ignore[attr-defined]\n",
      "\n",
      "        def load(module, local_state_dict, prefix=''):\n",
      "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
      "            if assign:\n",
      "                local_metadata['assign_to_params_buffers'] = assign\n",
      "            module._load_from_state_dict(\n",
      "                local_state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
      "            for name, child in module._modules.items():\n",
      "                if child is not None:\n",
      "                    child_prefix = prefix + name + '.'\n",
      "                    child_state_dict = {k: v for k, v in local_state_dict.items() if k.startswith(child_prefix)}\n",
      "                    load(child, child_state_dict, child_prefix)\n",
      "\n",
      "            # Note that the hook can modify missing_keys and unexpected_keys.\n",
      "            incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "            for hook in module._load_state_dict_post_hooks.values():\n",
      "                out = hook(module, incompatible_keys)\n",
      "                assert out is None, (\n",
      "                    \"Hooks registered with ``register_load_state_dict_post_hook`` are not\"\n",
      "                    \"expected to return new values, if incompatible_keys need to be modified,\"\n",
      "                    \"it should be done inplace.\"\n",
      "                )\n",
      "\n",
      "        load(self, state_dict)\n",
      "        del load\n",
      "\n",
      "        if strict:\n",
      "            if len(unexpected_keys) > 0:\n",
      "                error_msgs.insert(\n",
      "                    0, 'Unexpected key(s) in state_dict: {}. '.format(\n",
      "                        ', '.join(f'\"{k}\"' for k in unexpected_keys)))\n",
      "            if len(missing_keys) > 0:\n",
      "                error_msgs.insert(\n",
      "                    0, 'Missing key(s) in state_dict: {}. '.format(\n",
      "                        ', '.join(f'\"{k}\"' for k in missing_keys)))\n",
      "\n",
      "        if len(error_msgs) > 0:\n",
      "            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "                               self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "        return _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\n",
      "    def _named_members(self, get_members_fn, prefix='', recurse=True, remove_duplicate: bool = True):\n",
      "        r\"\"\"Helper method for yielding various names + members of modules.\"\"\"\n",
      "        memo = set()\n",
      "        modules = self.named_modules(prefix=prefix, remove_duplicate=remove_duplicate) if recurse else [(prefix, self)]\n",
      "        for module_prefix, module in modules:\n",
      "            members = get_members_fn(module)\n",
      "            for k, v in members:\n",
      "                if v is None or v in memo:\n",
      "                    continue\n",
      "                if remove_duplicate:\n",
      "                    memo.add(v)\n",
      "                name = module_prefix + ('.' if module_prefix else '') + k\n",
      "                yield name, v\n",
      "\n",
      "    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
      "        r\"\"\"Returns an iterator over module parameters.\n",
      "\n",
      "        This is typically passed to an optimizer.\n",
      "\n",
      "        Args:\n",
      "            recurse (bool): if True, then yields parameters of this module\n",
      "                and all submodules. Otherwise, yields only parameters that\n",
      "                are direct members of this module.\n",
      "\n",
      "        Yields:\n",
      "            Parameter: module parameter\n",
      "\n",
      "        Example::\n",
      "\n",
      "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "            >>> for param in model.parameters():\n",
      "            >>>     print(type(param), param.size())\n",
      "            <class 'torch.Tensor'> (20L,)\n",
      "            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "\n",
      "        \"\"\"\n",
      "        for name, param in self.named_parameters(recurse=recurse):\n",
      "            yield param\n",
      "\n",
      "    def named_parameters(\n",
      "            self,\n",
      "            prefix: str = '',\n",
      "            recurse: bool = True,\n",
      "            remove_duplicate: bool = True\n",
      "    ) -> Iterator[Tuple[str, Parameter]]:\n",
      "        r\"\"\"Returns an iterator over module parameters, yielding both the\n",
      "        name of the parameter as well as the parameter itself.\n",
      "\n",
      "        Args:\n",
      "            prefix (str): prefix to prepend to all parameter names.\n",
      "            recurse (bool): if True, then yields parameters of this module\n",
      "                and all submodules. Otherwise, yields only parameters that\n",
      "                are direct members of this module.\n",
      "            remove_duplicate (bool, optional): whether to remove the duplicated\n",
      "                parameters in the result. Defaults to True.\n",
      "\n",
      "        Yields:\n",
      "            (str, Parameter): Tuple containing the name and parameter\n",
      "\n",
      "        Example::\n",
      "\n",
      "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "            >>> for name, param in self.named_parameters():\n",
      "            >>>     if name in ['bias']:\n",
      "            >>>         print(param.size())\n",
      "\n",
      "        \"\"\"\n",
      "        gen = self._named_members(\n",
      "            lambda module: module._parameters.items(),\n",
      "            prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n",
      "        yield from gen\n",
      "\n",
      "    def buffers(self, recurse: bool = True) -> Iterator[Tensor]:\n",
      "        r\"\"\"Returns an iterator over module buffers.\n",
      "\n",
      "        Args:\n",
      "            recurse (bool): if True, then yields buffers of this module\n",
      "                and all submodules. Otherwise, yields only buffers that\n",
      "                are direct members of this module.\n",
      "\n",
      "        Yields:\n",
      "            torch.Tensor: module buffer\n",
      "\n",
      "        Example::\n",
      "\n",
      "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "            >>> for buf in model.buffers():\n",
      "            >>>     print(type(buf), buf.size())\n",
      "            <class 'torch.Tensor'> (20L,)\n",
      "            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "\n",
      "        \"\"\"\n",
      "        for _, buf in self.named_buffers(recurse=recurse):\n",
      "            yield buf\n",
      "\n",
      "    def named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, Tensor]]:\n",
      "        r\"\"\"Returns an iterator over module buffers, yielding both the\n",
      "        name of the buffer as well as the buffer itself.\n",
      "\n",
      "        Args:\n",
      "            prefix (str): prefix to prepend to all buffer names.\n",
      "            recurse (bool, optional): if True, then yields buffers of this module\n",
      "                and all submodules. Otherwise, yields only buffers that\n",
      "                are direct members of this module. Defaults to True.\n",
      "            remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      "\n",
      "        Yields:\n",
      "            (str, torch.Tensor): Tuple containing the name and buffer\n",
      "\n",
      "        Example::\n",
      "\n",
      "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "            >>> for name, buf in self.named_buffers():\n",
      "            >>>     if name in ['running_var']:\n",
      "            >>>         print(buf.size())\n",
      "\n",
      "        \"\"\"\n",
      "        gen = self._named_members(\n",
      "            lambda module: module._buffers.items(),\n",
      "            prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n",
      "        yield from gen\n",
      "\n",
      "    def children(self) -> Iterator['Module']:\n",
      "        r\"\"\"Returns an iterator over immediate children modules.\n",
      "\n",
      "        Yields:\n",
      "            Module: a child module\n",
      "        \"\"\"\n",
      "        for name, module in self.named_children():\n",
      "            yield module\n",
      "\n",
      "    def named_children(self) -> Iterator[Tuple[str, 'Module']]:\n",
      "        r\"\"\"Returns an iterator over immediate children modules, yielding both\n",
      "        the name of the module as well as the module itself.\n",
      "\n",
      "        Yields:\n",
      "            (str, Module): Tuple containing a name and child module\n",
      "\n",
      "        Example::\n",
      "\n",
      "            >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "            >>> for name, module in model.named_children():\n",
      "            >>>     if name in ['conv4', 'conv5']:\n",
      "            >>>         print(module)\n",
      "\n",
      "        \"\"\"\n",
      "        memo = set()\n",
      "        for name, module in self._modules.items():\n",
      "            if module is not None and module not in memo:\n",
      "                memo.add(module)\n",
      "                yield name, module\n",
      "\n",
      "    def modules(self) -> Iterator['Module']:\n",
      "        r\"\"\"Returns an iterator over all modules in the network.\n",
      "\n",
      "        Yields:\n",
      "            Module: a module in the network\n",
      "\n",
      "        Note:\n",
      "            Duplicate modules are returned only once. In the following\n",
      "            example, ``l`` will be returned only once.\n",
      "\n",
      "        Example::\n",
      "\n",
      "            >>> l = nn.Linear(2, 2)\n",
      "            >>> net = nn.Sequential(l, l)\n",
      "            >>> for idx, m in enumerate(net.modules()):\n",
      "            ...     print(idx, '->', m)\n",
      "\n",
      "            0 -> Sequential(\n",
      "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "            )\n",
      "            1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "\n",
      "        \"\"\"\n",
      "        for _, module in self.named_modules():\n",
      "            yield module\n",
      "\n",
      "    def named_modules(self, memo: Optional[Set['Module']] = None, prefix: str = '', remove_duplicate: bool = True):\n",
      "        r\"\"\"Returns an iterator over all modules in the network, yielding\n",
      "        both the name of the module as well as the module itself.\n",
      "\n",
      "        Args:\n",
      "            memo: a memo to store the set of modules already added to the result\n",
      "            prefix: a prefix that will be added to the name of the module\n",
      "            remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "                or not\n",
      "\n",
      "        Yields:\n",
      "            (str, Module): Tuple of name and module\n",
      "\n",
      "        Note:\n",
      "            Duplicate modules are returned only once. In the following\n",
      "            example, ``l`` will be returned only once.\n",
      "\n",
      "        Example::\n",
      "\n",
      "            >>> l = nn.Linear(2, 2)\n",
      "            >>> net = nn.Sequential(l, l)\n",
      "            >>> for idx, m in enumerate(net.named_modules()):\n",
      "            ...     print(idx, '->', m)\n",
      "\n",
      "            0 -> ('', Sequential(\n",
      "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "            ))\n",
      "            1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        if memo is None:\n",
      "            memo = set()\n",
      "        if self not in memo:\n",
      "            if remove_duplicate:\n",
      "                memo.add(self)\n",
      "            yield prefix, self\n",
      "            for name, module in self._modules.items():\n",
      "                if module is None:\n",
      "                    continue\n",
      "                submodule_prefix = prefix + ('.' if prefix else '') + name\n",
      "                yield from module.named_modules(memo, submodule_prefix, remove_duplicate)\n",
      "\n",
      "    def train(self: T, mode: bool = True) -> T:\n",
      "        r\"\"\"Sets the module in training mode.\n",
      "\n",
      "        This has any effect only on certain modules. See documentations of\n",
      "        particular modules for details of their behaviors in training/evaluation\n",
      "        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "        etc.\n",
      "\n",
      "        Args:\n",
      "            mode (bool): whether to set training mode (``True``) or evaluation\n",
      "                         mode (``False``). Default: ``True``.\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        if not isinstance(mode, bool):\n",
      "            raise ValueError(\"training mode is expected to be boolean\")\n",
      "        self.training = mode\n",
      "        for module in self.children():\n",
      "            module.train(mode)\n",
      "        return self\n",
      "\n",
      "    def eval(self: T) -> T:\n",
      "        r\"\"\"Sets the module in evaluation mode.\n",
      "\n",
      "        This has any effect only on certain modules. See documentations of\n",
      "        particular modules for details of their behaviors in training/evaluation\n",
      "        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "        etc.\n",
      "\n",
      "        This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "\n",
      "        See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "        `.eval()` and several similar mechanisms that may be confused with it.\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        return self.train(False)\n",
      "\n",
      "    def requires_grad_(self: T, requires_grad: bool = True) -> T:\n",
      "        r\"\"\"Change if autograd should record operations on parameters in this\n",
      "        module.\n",
      "\n",
      "        This method sets the parameters' :attr:`requires_grad` attributes\n",
      "        in-place.\n",
      "\n",
      "        This method is helpful for freezing part of the module for finetuning\n",
      "        or training parts of a model individually (e.g., GAN training).\n",
      "\n",
      "        See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "        `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "\n",
      "        Args:\n",
      "            requires_grad (bool): whether autograd should record operations on\n",
      "                                  parameters in this module. Default: ``True``.\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \"\"\"\n",
      "        for p in self.parameters():\n",
      "            p.requires_grad_(requires_grad)\n",
      "        return self\n",
      "\n",
      "    def zero_grad(self, set_to_none: bool = True) -> None:\n",
      "        r\"\"\"Resets gradients of all model parameters. See similar function\n",
      "        under :class:`torch.optim.Optimizer` for more context.\n",
      "\n",
      "        Args:\n",
      "            set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "                See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "        \"\"\"\n",
      "        if getattr(self, '_is_replica', False):\n",
      "            warnings.warn(\n",
      "                \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \"\n",
      "                \"The parameters are copied (in a differentiable manner) from the original module. \"\n",
      "                \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \"\n",
      "                \"If you need gradients in your forward method, consider using autograd.grad instead.\")\n",
      "\n",
      "        for p in self.parameters():\n",
      "            if p.grad is not None:\n",
      "                if set_to_none:\n",
      "                    p.grad = None\n",
      "                else:\n",
      "                    if p.grad.grad_fn is not None:\n",
      "                        p.grad.detach_()\n",
      "                    else:\n",
      "                        p.grad.requires_grad_(False)\n",
      "                    p.grad.zero_()\n",
      "\n",
      "    def share_memory(self: T) -> T:\n",
      "        r\"\"\"See :meth:`torch.Tensor.share_memory_`\"\"\"\n",
      "        return self._apply(lambda t: t.share_memory_())\n",
      "\n",
      "    def _get_name(self):\n",
      "        return self.__class__.__name__\n",
      "\n",
      "    def extra_repr(self) -> str:\n",
      "        r\"\"\"Set the extra representation of the module\n",
      "\n",
      "        To print customized extra information, you should re-implement\n",
      "        this method in your own modules. Both single-line and multi-line\n",
      "        strings are acceptable.\n",
      "        \"\"\"\n",
      "        return ''\n",
      "\n",
      "    def __repr__(self):\n",
      "        # We treat the extra repr like the sub-module, one item per line\n",
      "        extra_lines = []\n",
      "        extra_repr = self.extra_repr()\n",
      "        # empty string will be split into list ['']\n",
      "        if extra_repr:\n",
      "            extra_lines = extra_repr.split('\\n')\n",
      "        child_lines = []\n",
      "        for key, module in self._modules.items():\n",
      "            mod_str = repr(module)\n",
      "            mod_str = _addindent(mod_str, 2)\n",
      "            child_lines.append('(' + key + '): ' + mod_str)\n",
      "        lines = extra_lines + child_lines\n",
      "\n",
      "        main_str = self._get_name() + '('\n",
      "        if lines:\n",
      "            # simple one-liner info, which most builtin Modules will use\n",
      "            if len(extra_lines) == 1 and not child_lines:\n",
      "                main_str += extra_lines[0]\n",
      "            else:\n",
      "                main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n",
      "\n",
      "        main_str += ')'\n",
      "        return main_str\n",
      "\n",
      "    def __dir__(self):\n",
      "        module_attrs = dir(self.__class__)\n",
      "        attrs = list(self.__dict__.keys())\n",
      "        parameters = list(self._parameters.keys())\n",
      "        modules = list(self._modules.keys())\n",
      "        buffers = list(self._buffers.keys())\n",
      "        keys = module_attrs + attrs + parameters + modules + buffers\n",
      "\n",
      "        # Eliminate attrs that are not legal Python variable names\n",
      "        keys = [key for key in keys if not key[0].isdigit()]\n",
      "\n",
      "        return sorted(keys)\n",
      "\n",
      "    def _replicate_for_data_parallel(self):\n",
      "        replica = self.__new__(type(self))\n",
      "        replica.__dict__ = self.__dict__.copy()\n",
      "\n",
      "        # replicas do not have parameters themselves, the replicas reference the original\n",
      "        # module.\n",
      "        replica._parameters = OrderedDict()\n",
      "        replica._buffers = replica._buffers.copy()\n",
      "        replica._modules = replica._modules.copy()\n",
      "        replica._is_replica = True  # type: ignore[assignment]\n",
      "\n",
      "        return replica\n",
      "\n",
      "    def compile(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Compile this Module's forward using :func:`torch.compile`.\n",
      "\n",
      "        This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      "        to :func:`torch.compile`.\n",
      "\n",
      "        See :func:`torch.compile` for details on the arguments for this function.\n",
      "        \"\"\"\n",
      "        self._compiled_call_impl = torch.compile(self._call_impl, *args, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect # 파이썬 내부 클래스나 함수의 소스 코드 확인 할 수 있음\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "print(inspect.getsource(nn.Module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel(\n",
      "  (layer): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "tensor([[  1.5358],\n",
      "        [ 69.2628],\n",
      "        [357.2008]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"mps\")\n",
    "model = torch.load(\"./models/model.pt\", map_location=device) # 저장된 클래스 이름에 해당하는 실제 클래스 정의를 현재 환경에서 찾기 때문에 위에 CustomModel 선언\n",
    "print(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    inputs = torch.FloatTensor(\n",
    "        [\n",
    "            [1 ** 2, 1],\n",
    "            [5 ** 2, 5],\n",
    "            [11 ** 2, 11]\n",
    "        ]\n",
    "    ).to(device)\n",
    "    outputs = model(inputs)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel(\n",
      "  (layer): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조를 모르겠다면 모델 구조를 출력할 수 있음\n",
    "# 변수의 명칭까지 동일한 형태로 구현\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    pass\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model = torch.load(\"./models/model.pt\", map_location=device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 상태 저장/불러오기|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 상태만 저장\n",
    "import torch\n",
    "\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    \"./models/model_state_dict.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('layer.weight', tensor([[ 3.1058, -1.7030]], device='mps:0')), ('layer.bias', tensor([0.1329], device='mps:0'))])\n"
     ]
    }
   ],
   "source": [
    "# 즉 가중치와 편향이 저장되어 있음\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 상태 불러오기\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device('mps')\n",
    "model = CustomModel().to(device)\n",
    "\n",
    "model_state_dict = torch.load('./models/model_state_dict.pt', map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    inputs = torch.FloatTensor(\n",
    "        [\n",
    "            [1 ** 2, 1],\n",
    "            [5 ** 2, 5],\n",
    "            [11 ** 2, 11]\n",
    "        ]\n",
    "    ).to(device)\n",
    "    outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 체크포인트 저장/불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_py3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([128, 2])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        self.x = df.iloc[:, 0].values\n",
    "        self.y = df.iloc[:, 1].values\n",
    "        self.length = len(df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.FloatTensor([self.x[index] ** 2, self.x[index]])\n",
    "        y = torch.FloatTensor([self.y[index]])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "    \n",
    "train_dataset = CustomDataset(\"./dataset/non_linear.csv\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "device = torch.device('mps')\n",
    "model = CustomModel().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "checkpoint = 1\n",
    "for epoch in range(10000):\n",
    "    cost = 0.0\n",
    "\n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = x.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss\n",
    "\n",
    "    cost = cost / len(train_dataloader)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": \"CustomModel\", # 모델 식별자\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"cost\": cost,\n",
    "                \"description\": f\"CustomModel 체크포인트-{checkpoint}\",\n",
    "            },\n",
    "            f'./models/checkpoint-{checkpoint}.pt',\n",
    "        )\n",
    "\n",
    "        checkpoint += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel 체크포인트-6\n",
      "Epoch: 7000, Model : [Parameter containing:\n",
      "tensor([[ 3.1043, -1.7002]], device='mps:0', requires_grad=True), Parameter containing:\n",
      "tensor([0.2413], device='mps:0', requires_grad=True)], Cost: 0.095\n",
      "Epoch: 8000, Model : [Parameter containing:\n",
      "tensor([[ 3.1040, -1.7029]], device='mps:0', requires_grad=True), Parameter containing:\n",
      "tensor([0.2628], device='mps:0', requires_grad=True)], Cost: 0.102\n",
      "Epoch: 9000, Model : [Parameter containing:\n",
      "tensor([[ 3.1037, -1.7032]], device='mps:0', requires_grad=True), Parameter containing:\n",
      "tensor([0.2825], device='mps:0', requires_grad=True)], Cost: 0.100\n",
      "Epoch: 10000, Model : [Parameter containing:\n",
      "tensor([[ 3.1034, -1.7031]], device='mps:0', requires_grad=True), Parameter containing:\n",
      "tensor([0.3004], device='mps:0', requires_grad=True)], Cost: 0.093\n"
     ]
    }
   ],
   "source": [
    "### 체크포인트 불러오기\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        self.x = df.iloc[:, 0].values\n",
    "        self.y = df.iloc[:, 1].values\n",
    "        self.length = len(df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.FloatTensor([self.x[index] ** 2, self.x[index]])\n",
    "        x = torch.FloatTensor([self.y[index]])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "    \n",
    "checkpoint = torch.load(\"./models/checkpoint-6.pt\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"]) # 위에 선언한 model_state_dict 선언\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"]) # 위에 선언한 optimizer_state_dict 선언\n",
    "checkpoint_epoch = checkpoint[\"epoch\"]\n",
    "checkpoint_description = checkpoint[\"description\"]\n",
    "print(checkpoint_description)\n",
    "\n",
    "for epoch in range(checkpoint_epoch + 1, 10000): # 체크 포인트 시점 점 부터\n",
    "    cost = 0.0\n",
    "\n",
    "    for x,y in train_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output= model(x)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f'Epoch: {epoch+1:4d}, Model : {list(model.parameters())}, Cost: {cost:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
