{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 정칙화 적용 방식 (주로 선형 모델에 적용, )\n",
    "import torch\n",
    "\n",
    "device = torch.device('mps')\n",
    "for x, y in train_dataloader:\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    output = model(x)\n",
    "\n",
    "    _lambda = 0.5\n",
    "    l1_loss = sum(p.abs().sum() for p in model.parameters()) # 가중치의 절대 값 합을 사용\n",
    "\n",
    "    loss = criterion(output, y) + _lambda * l1_loss # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 정칙화 적용 방식 (심층 신경망에 사용)\n",
    "\n",
    "device = torch.device('mps')\n",
    "for x, y in train_dataloader:\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    output = model(x)\n",
    "\n",
    "    _lambda = 0.5\n",
    "    l2_loss = sum(p.pow(2.0).sum() for p in model.parameters()) # 가중치의 제곱의 합을 사용\n",
    "\n",
    "    loss = criterion(output, y) + _lambda * l2_loss # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 감쇠 적용 방식\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= 0.01, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모멘텀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엘라스틱 넷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃 (일반적으로 배치 정규화와 동시에 사용하지 않음)\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 10)\n",
    "        self.dropout = nn.Dropout(p=0.5) # p는 베르누이 분포의 모수 의미 (노드의 제거 여부를 확률적으로 선택)\n",
    "        self.layer2 = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그레이디언트 클리핑 (모델을 학습할 때 기울기가 너무 커지는 현상을 방지하는 기술, 순환 신경망이나 LSTM모델 학습때 주로 사용)\n",
    "\n",
    "for x, y in train_dataloader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn,utils.clip_grad_norm_(model.parameters(), 0.1) # 역전파 수행 이후 최적화 함수 반영 전에 호출\n",
    "\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
